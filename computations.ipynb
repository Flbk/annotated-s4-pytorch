{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\CC}{\\mathbb{C}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\Zz}{\\mathcal{Z}}\n",
    "\\newcommand{\\Ww}{\\mathcal{W}}\n",
    "\\newcommand{\\Vv}{\\mathcal{V}}\n",
    "\\newcommand{\\Nn}{\\mathcal{N}}\n",
    "\\newcommand{\\NN}{\\mathcal{N}}\n",
    "\\newcommand{\\Hh}{\\mathcal{H}}\n",
    "\\newcommand{\\Bb}{\\mathcal{B}}\n",
    "\\newcommand{\\Ee}{\\mathcal{E}}\n",
    "\\newcommand{\\Cc}{\\mathcal{C}}\n",
    "\\newcommand{\\Gg}{\\mathcal{G}}\n",
    "\\newcommand{\\Ss}{\\mathcal{S}}\n",
    "\\newcommand{\\Pp}{\\mathcal{P}}\n",
    "\\newcommand{\\Ff}{\\mathcal{F}}\n",
    "\\newcommand{\\Xx}{\\mathcal{X}}\n",
    "\\newcommand{\\Mm}{\\mathcal{M}}\n",
    "\\newcommand{\\Ii}{\\mathcal{I}}\n",
    "\\newcommand{\\Dd}{\\mathcal{D}}\n",
    "\\newcommand{\\Ll}{\\mathcal{L}}\n",
    "\\newcommand{\\Tt}{\\mathcal{T}}\n",
    "\\newcommand{\\al}{\\alpha}\n",
    "\\newcommand{\\la}{\\lambda}\n",
    "\\newcommand{\\ga}{\\gamma}\n",
    "\\newcommand{\\Ga}{\\Gamma}\n",
    "\\newcommand{\\La}{\\Lambda}\n",
    "\\newcommand{\\si}{\\sigma}\n",
    "\\newcommand{\\Si}{\\Sigma}\n",
    "\\newcommand{\\be}{\\beta}\n",
    "\\newcommand{\\de}{\\delta}\n",
    "\\newcommand{\\De}{\\Delta}\n",
    "\\renewcommand{\\phi}{\\varphi}\n",
    "\\renewcommand{\\th}{\\theta}\n",
    "\\newcommand{\\om}{\\omega}\n",
    "\\newcommand{\\Om}{\\Omega}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\newcommand{\\bo}[1]{\\boldsymbol{\\mathbf{#1}}}\n",
    "\\newcommand{\\bu}{\\bo{u}}\n",
    "\\newcommand{\\bv}{\\bo{v}}\n",
    "\\newcommand{\\bC}{\\bo{C}}\n",
    "\\newcommand{\\bp}{\\bo{p}}\n",
    "\\newcommand{\\bq}{\\bo{q}}\n",
    "\\newcommand{\\bX}{\\bo{X}}\n",
    "\\newcommand{\\bc}{\\bo{c}}\n",
    "\\newcommand{\\bb}{\\bo{b}}\n",
    "\\newcommand{\\bh}{\\bo{h}}\n",
    "\\newcommand{\\by}{\\bo{y}}\n",
    "\\newcommand{\\lc}{\\bo{L}(\\bC)}\n",
    "\\newcommand{\\lcb}[1]{\\bo{L}(\\bo{#1})}\n",
    "\\newcommand{\\ba}{\\bo{a}}\n",
    "\\newcommand{\\bbv}{\\bo{b}}\n",
    "\\newcommand{\\tD}{\\bo{\\widetilde{D}}}\n",
    "\\newcommand{\\tLa}{\\bo{\\widetilde{\\La}}}\n",
    "\\newcommand{\\tCbe}{\\bo{\\widetilde{C}^{\\be}}}\n",
    "\\newcommand{\\bbe}{\\bo{\\beta}}\n",
    "\\newcommand{\\Cbe}{\\bC^{\\bbe}}\n",
    "\\newcommand{\\bhat}{\\bo{\\hat{\\be}}}\n",
    "\\newcommand{\\pibe}{\\bpi^{\\bbe}}\n",
    "\\newcommand{\\bD}{\\bo{D}}\n",
    "\\newcommand{\\bZ}{\\bo{Z}}\n",
    "\\newcommand{\\bF}{\\bo{F}}\n",
    "\\newcommand{\\bA}{\\bo{A}}\n",
    "\\newcommand{\\bB}{\\bo{B}}\n",
    "\\newcommand{\\bK}{\\bo{K}}\n",
    "\\newcommand{\\bI}{\\bo{I}}\n",
    "\\newcommand{\\bS}{\\bo{S}}\n",
    "\\newcommand{\\bLa}{\\bo{\\La}}\n",
    "\\newcommand{\\ctilde}{\\bo{\\tilde{c}}}\n",
    "\\newcommand{\\bhatK}{\\bo{\\hat{K}}}\n",
    "\\newcommand{\\bR}{\\bo{R}}\n",
    "\\newcommand{\\bAb}{\\bo{\\bar{A}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 in Pytorch.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook aims at explaining the S4 layer described in [S4](https://arxiv.org/abs/2111.00396), with more details on practical use cases, as well as a simple Pytorch implementation.\n",
    "Some parts of this code are directly coming from the excellent guide [Annotated S4](https://srush.github.io/annotated-s4/).\n",
    "\n",
    "The goal of the notebook is to be as concise as possible while providing enough details for a full comprehension of a practical implementation of S4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation\n",
    "\n",
    "### Classical approaches\n",
    "\n",
    "Deep NLP has made huge progress since the arrival of the sequence-to-sequence models. A sequence-to-sequence model is a model that maps a sequence $u_0, \\dots, u_{L-1}$ to another sequence $y_0, \\dots, y_{L-1}$.\n",
    "\n",
    "The main approaches in NLP were first RNN, that mimics Hidden Markov Process. In simple terms, a RNN models a situation recursively:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        h_{k+1} &=& g(u_{k+1}, h_k),\\\\\n",
    "        y_{k+1} &=& f(h_{k+1}).\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "where $h_k$ represents a hidden state. Here each output $y_k$ is built recursively on the precedent value $h_{k-1}$. The main drawbacks of such models is that it is extremely difficult control how the information of previous steps flows up to the current step. It has even been derived that statistically such networks doesn't have long-memory (i.e the output at step $k$ is unlikely to have retained information of $u_{k-l}$if $l$ is large enough).\n",
    "\n",
    "The other and currently SOTA architecture is the transformer.\n",
    "\n",
    "Transformers are extremelly powerful because the output sequence depends on *all* the previous steps:\n",
    "\n",
    "\\begin{equation}\n",
    "    y_{k+1} = \\sum_{i=0}^{k} f(u_i, u_{k+1}).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore they do have long-memory. But here the problem comes form the memory consumption of such operation. To process a full sequence the memory complexity is $\\mathcal{O}(L^2)$.\n",
    "\n",
    "### State-space approach\n",
    "\n",
    "To overcome these problem, S4 introduces the formalism of state-space models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_init(N):\n",
    "    u = torch.arange(N)\n",
    "    b = torch.sqrt(2 * u[:, None] + 1)\n",
    "    A = b @ b.T\n",
    "    A = torch.tril(A, 0)\n",
    "    A = - (A - torch.diag(u))\n",
    "\n",
    "\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = hippo_init(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_init_dplr(N):\n",
    "    u = torch.arange(N)[:, None]\n",
    "    \n",
    "    p = torch.sqrt(u + 0.5)\n",
    "\n",
    "    # Extract the skew-hermitian part of A.\n",
    "    S = torch.tril(p @ p.T)\n",
    "    S = - S + S.T\n",
    "\n",
    "    # A small trick to make a skew-hermitian matrix a hermitian one.\n",
    "    hermitian_S = S * -1j\n",
    "    Lambda, V = torch.linalg.eigh(hermitian_S)\n",
    "\n",
    "    # Mutliplies back the eigenvalues by (1j)^-1 to retrieve the original eigenvalues of the skew-hermitian matrix.\n",
    "    # We have to add the real parts of the eigenvalues, coming for the 1/2*Id part of the decomposition. \n",
    "    Lambda = Lambda * 1j - 0.5\n",
    "\n",
    "    # Change of basis for b and p.\n",
    "    b = torch.sqrt(2 * u + 1)\n",
    "    b = V.H @ b.type(torch.complex64)\n",
    "    p = V.H @ p.type(torch.complex64)\n",
    "    \n",
    "    return V, Lambda, p, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, Lambda, p, b = hippo_init_dplr(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(V @ (torch.diag(Lambda) - p @ p.H) @ V.H, A.type(torch.complex64), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "The naive way to make the convolution would be the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_conv(A, b, c, L):\n",
    "    return torch.tensor([(c.T @ A.matrix_power(l) @ b).item() for l in range(L)]).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "L = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(N, N)\n",
    "c = torch.randn(N, 1)\n",
    "b = torch.randn(N, 1)\n",
    "u = torch.randn(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = k_conv(A, b, c, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution(u, K):\n",
    "    u_fft = torch.fft.fft(F.pad(u, (0, K.shape[0])))\n",
    "    K_fft = torch.fft.fft(F.pad(K, (0, u.shape[0])))\n",
    "    out = u_fft * K_fft\n",
    "    return torch.fft.ifft(out).real[: u.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4.8000,   7.1977,  32.7416,   3.7354, 233.8863])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_convolution(u, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the main bottleneck of this computation is how we obtain the convolution kernel, because we are exponentiating a matrix L time, compute a lot of matrix products etc, which is highly non efficient.\n",
    "\n",
    "To make this more efficient, we can find more sophisticated algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namely, instead of computing directly $\\bK$, we are going to compute its spectrum, $\\hat{\\bK}$ and then simply apply a inverse Fourier Transform (IDFT). Let $\\om_k = \\exp\\left(-{\\dfrac{2i \\pi k}{N}}\\right)$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\bK}_k &= \\sum_{i=0}^{L-1} \\bK_i \\omega_k^i,\\\\\n",
    "    &= \\sum \\bc^T A^i b \\omega_k^i,\\\\\n",
    "    &= \\bc^T \\left(\\sum \\bA^i \\omega_k^i \\right) \\bb,\\\\\n",
    "    &= \\bc^T (\\bI - \\bA^L)(\\bI - \\bA\\omega_k)^{-1} \\bb.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this does not really reduce the complexity, because now we have to compute an inverse $L$ times (for each $\\om_k$), giving a complexity of $\\mathcal{O}(LN^3)$.\n",
    "\n",
    "\n",
    "However, we are lucky because we are in fact dealing with matrices that have a special form: diagonal + low-rank.\n",
    "\n",
    "Indeed, the matrix $A$ we are using can be decomposed in the following form:\n",
    "\\begin{equation}\n",
    "    \\bA = - \\dfrac{1}{2}\\bI - \\bS - \\bp^*\\bp,\n",
    "\\end{equation}\n",
    "where $\\bS$ is a skew-hermitian matrix ($\\bS = - \\bS^*$), and $\\bp \\in \\RR^{N}$ is a vector of dimension 1.\n",
    "\n",
    "And in the adapted basis, the inverse of a DLPR matrix is much more easy to compute, thanks to the Woodburry inversion formula:\n",
    "\n",
    "\\begin{equation}\n",
    "    (\\bLa - \\bp^* \\bq)^{-1} = \\bLa^{-1} + \\bLa^{-1}\\bp \\left(1 - \\bq^*\\bLa^{-1} \\bp\\right)^{-1}\\bq^* \\bLa^{-1}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this result to the expression of $\\bhatK$, with some more algebraic operations, we obtain a quite simple formula for its expression:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bhatK_k = \\dfrac{2}{1 + \\om_k}\\left[\\ctilde^* \\bR_k \\bb - \\ctilde^* \\bR_k \\bp (1 + \\bp^*\\bR_k \\bp)^{-1}\\bp^*\\bR_k \\bb\\right].\n",
    "\\end{equation}\n",
    "\n",
    "with $\\bR_k = \\left(2\\dfrac{1 - \\om_k}{1 + \\om_k} - \\bLa \\right)^{-1}$ and $\\ctilde^* = \\bc^* (\\bI - \\bAb^L)$.\n",
    "\n",
    "And here, in fact all the matrix multiplications can be evaluated very efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   \\ctilde^T \\bR_k \\bb &= \\sum_{i=1}^{N} \\dfrac{\\ctilde_i \\bb_i}{\\om_k - \\bLa_i},\n",
    "\\end{align}\n",
    "\n",
    "and because we want to compute it for all $\\om_k$, the complexity reduces to $\\mathcal{O}(NL)$.\n",
    "In fact, we can be even more efficient, be we need to rely on algorithms that are not yet implemented on Pytorch.\n",
    "But notice that this complexity is not a bottleneck when the biggest memory cost comes from the size of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy(p, q, lambd, omega_L):\n",
    "    omega_L = 2. * ((1. - omega_L) / (1 + omega_L))\n",
    "    dot_product = p * q\n",
    "    lambd = lambd[:, None]\n",
    "    omega_L = omega_L[None, :]\n",
    "    cauchy_product = dot_product / (omega_L - lambd)\n",
    "    return cauchy_product.sum(axis=-2).view(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_dplr(lambd, b, c, p, L):\n",
    "    omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "    term_1 = cauchy(b, c.conj(), lambd, omega_L)\n",
    "    term_2 = cauchy(c.conj(), p, lambd, omega_L)\n",
    "    term_3 = 1. / (1. + cauchy(p.conj(), p, lambd, omega_L))\n",
    "    term_4 = cauchy(p.conj(), b, lambd, omega_L)\n",
    "    \n",
    "    K_fft = (2. / (1. + omega_L)) * (term_1 - term_2 * term_3 * term_4) \n",
    "    return torch.fft.ifft(K_fft, L).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gen_simple(A, b, c, L):\n",
    "    K = k_conv(A, b, c, L)\n",
    "    def gen(z):\n",
    "        return torch.sum(K *  z ** torch.arange(L))\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gen_inverse(A, b, ct, L):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    A_L = A.matrix_power(L)\n",
    "    return lambda z: (ct.H @ torch.linalg.inv(I - A * z) @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_from_gen(A, b, c, L, inv=False):\n",
    "    if not inv:\n",
    "        gen = k_gen_simple(A, b, c, L)\n",
    "    else:\n",
    "        gen = k_gen_inverse(A, b, c, L)\n",
    "    omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "    at_roots = torch.tensor([gen(z) for z in omega_L])\n",
    "    out = torch.fft.ifft(at_roots, L).reshape(L)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(A, b, c, step=1):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    b_left = torch.linalg.inv(I - (step / 2.0) * A)\n",
    "    A_bar = b_left @ (I + (step / 2.0) * A)\n",
    "    b_bar = (b_left * step) @ b\n",
    "    return A_bar, b_bar, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv_kernel(L=10, N=20):\n",
    "    \n",
    "    V, Lambda, p, b = hippo_init_dplr(N)\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    c = torch.randn(N, 1).type(torch.complex64)\n",
    "    c_basis_diag = V.H @ c.type(torch.complex64)\n",
    "    A_bar, b_bar, c_bar = discretize(A, b, c_basis_diag)\n",
    "    c_tilde_h = c_bar.T @ (torch.eye(N) - A_bar.matrix_power(L))\n",
    "    c_tilde = c_tilde_h.H\n",
    "    K_simple = k_conv(A_bar, b_bar, c_bar, L)\n",
    "    K_efficient = kernel_dplr(Lambda, b, c_tilde, p, L)\n",
    "\n",
    "    assert torch.allclose(K_simple, K_efficient, atol=1e-4)\n",
    "    \n",
    "    u = torch.randn(L)\n",
    "    \n",
    "    y_simple = causal_convolution(u, K_simple)\n",
    "    y_efficient = causal_convolution(u, K_efficient)\n",
    "    assert torch.allclose(y_simple, y_efficient, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multidimensional_input_simple(u, K):\n",
    "    out = torch.zeros_like(u)\n",
    "    for i in range(u.shape[0]):\n",
    "        for j in range(u.shape[-1]):\n",
    "            out[i, :, j] = causal_convolution(u[i, :, j], K)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution_n(u, K):\n",
    "    K = K[None, :, None]\n",
    "    fft_shape = u.shape[1] + K.shape[1]\n",
    "    u_fft = torch.fft.fft(u, n=fft_shape, axis=1)\n",
    "    K_fft = torch.fft.fft(K, n=fft_shape, axis=1)\n",
    "    out = u_fft * K_fft\n",
    "    return torch.fft.ifft(out, axis=1)[:, :u.shape[1], :].real\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multidimensional_convolution(batch_size=8, seq_length=10, hidden_size=5):\n",
    "    u = torch.randn(batch_size, seq_length, hidden_size)\n",
    "    K = torch.randn(seq_length)\n",
    "    simple_conv = multidimensional_input_simple(u, K)\n",
    "    efficient_conv = causal_convolution_n(u, K)\n",
    "    assert torch.allclose(efficient_conv, simple_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multidimensional_convolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent decoding\n",
    "\n",
    "In a situation of unsupervised generation, we need to be able to compute the state space equation autoregressively.\n",
    "But hopefully, thanks to the parametrization of $\\bA$ as a NPLR matrix, we can compute a recurrence step in $\\mathcal{O}(NH)$ where $H$ is the hidden dimension and $N$ the polynomial space dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_dplr(Lambda, p, b, c, L, step=1.):\n",
    "    # Convert parameters to matrices\n",
    "    b = b\n",
    "\n",
    "    N = Lambda.shape[0]\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    I = torch.eye(N)\n",
    "\n",
    "    # Forward Euler\n",
    "    A0 = (2.0 / step) * I + A\n",
    "\n",
    "    # Backward Euler\n",
    "    D = torch.diag(1.0 / ((2.0 / step) - Lambda))\n",
    "    Dp = D @ p\n",
    "    \n",
    "    A1 = D - (Dp * (1.0 / (1 + (p.H @ Dp))) * p.H @ D)\n",
    "\n",
    "    # A bar and B bar\n",
    "    A_bar = A1 @ A0\n",
    "    b_bar = 2 * A1 @ b\n",
    "\n",
    "    return A_bar, b_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_ssm(Ab, Bb, Cb, u, x0):\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = Ab @ x_k_1 + Bb @ u_k\n",
    "        y_k = Cb @ x_k\n",
    "        return x_k, y_k\n",
    "    recurrence = []\n",
    "    x_k = x0\n",
    "    for i in range(u.shape[0]):\n",
    "        x_k, y_k = step(x_k, u[i])\n",
    "        recurrence.append(y_k)\n",
    "    return x_k, torch.tensor(recurrence).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conversion(N=8, L=16):\n",
    "    V, Lambda, p, b = hippo_init_dplr(N)\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    c = torch.randn(N, 1).type(torch.complex64)\n",
    "    c_basis_diag = V.H @ c.type(torch.complex64)\n",
    "    A_bar, b_bar, c_bar = discretize(A, b, c_basis_diag)\n",
    "    c_tilde_h = c_bar.T @ (torch.eye(N) - A_bar.matrix_power(L))\n",
    "    c_tilde = c_tilde_h.H\n",
    "\n",
    "    # CNN form.\n",
    "    K = kernel_dplr(Lambda, b, c_tilde, p, L)\n",
    "\n",
    "    # RNN form.\n",
    "    Ab, Bb = discrete_dplr(Lambda, p, b, c, L)\n",
    "    K2 = k_conv(A_bar, b_bar, c_bar, L=L)\n",
    "    assert np.allclose(K, K2, atol=1e-5, rtol=1e-5)\n",
    "\n",
    "    # Apply CNN\n",
    "    u = torch.arange(L).type(torch.complex64)\n",
    "    y1 = causal_convolution(u, K)\n",
    "\n",
    "    # Apply RNN\n",
    "    _, y2 = scan_ssm(\n",
    "        Ab, Bb, c_bar.T, u[:, np.newaxis], torch.zeros((N,)).type(torch.complex64)\n",
    "    )\n",
    "    assert np.allclose(y1, y2.reshape(-1).real, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conversion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-s4",
   "language": "python",
   "name": "env-s4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a263f42c39b2fd75b37cb89946a27f6140cd004b337ff228565a862a637fff96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
