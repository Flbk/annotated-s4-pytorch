{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\CC}{\\mathbb{C}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\Zz}{\\mathcal{Z}}\n",
    "\\newcommand{\\Ww}{\\mathcal{W}}\n",
    "\\newcommand{\\Vv}{\\mathcal{V}}\n",
    "\\newcommand{\\Nn}{\\mathcal{N}}\n",
    "\\newcommand{\\NN}{\\mathcal{N}}\n",
    "\\newcommand{\\Hh}{\\mathcal{H}}\n",
    "\\newcommand{\\Bb}{\\mathcal{B}}\n",
    "\\newcommand{\\Ee}{\\mathcal{E}}\n",
    "\\newcommand{\\Cc}{\\mathcal{C}}\n",
    "\\newcommand{\\Gg}{\\mathcal{G}}\n",
    "\\newcommand{\\Ss}{\\mathcal{S}}\n",
    "\\newcommand{\\Pp}{\\mathcal{P}}\n",
    "\\newcommand{\\Ff}{\\mathcal{F}}\n",
    "\\newcommand{\\Xx}{\\mathcal{X}}\n",
    "\\newcommand{\\Mm}{\\mathcal{M}}\n",
    "\\newcommand{\\Ii}{\\mathcal{I}}\n",
    "\\newcommand{\\Dd}{\\mathcal{D}}\n",
    "\\newcommand{\\Ll}{\\mathcal{L}}\n",
    "\\newcommand{\\Tt}{\\mathcal{T}}\n",
    "\\newcommand{\\al}{\\alpha}\n",
    "\\newcommand{\\la}{\\lambda}\n",
    "\\newcommand{\\ga}{\\gamma}\n",
    "\\newcommand{\\Ga}{\\Gamma}\n",
    "\\newcommand{\\La}{\\Lambda}\n",
    "\\newcommand{\\si}{\\sigma}\n",
    "\\newcommand{\\Si}{\\Sigma}\n",
    "\\newcommand{\\be}{\\beta}\n",
    "\\newcommand{\\de}{\\delta}\n",
    "\\newcommand{\\De}{\\Delta}\n",
    "\\renewcommand{\\phi}{\\varphi}\n",
    "\\renewcommand{\\th}{\\theta}\n",
    "\\newcommand{\\om}{\\omega}\n",
    "\\newcommand{\\Om}{\\Omega}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\newcommand{\\bo}[1]{\\boldsymbol{\\mathbf{#1}}}\n",
    "\\newcommand{\\bu}{\\bo{u}}\n",
    "\\newcommand{\\bv}{\\bo{v}}\n",
    "\\newcommand{\\bC}{\\bo{C}}\n",
    "\\newcommand{\\bp}{\\bo{p}}\n",
    "\\newcommand{\\bq}{\\bo{q}}\n",
    "\\newcommand{\\bX}{\\bo{X}}\n",
    "\\newcommand{\\bc}{\\bo{c}}\n",
    "\\newcommand{\\bb}{\\bo{b}}\n",
    "\\newcommand{\\bh}{\\bo{h}}\n",
    "\\newcommand{\\by}{\\bo{y}}\n",
    "\\newcommand{\\lc}{\\bo{L}(\\bC)}\n",
    "\\newcommand{\\lcb}[1]{\\bo{L}(\\bo{#1})}\n",
    "\\newcommand{\\ba}{\\bo{a}}\n",
    "\\newcommand{\\bbv}{\\bo{b}}\n",
    "\\newcommand{\\tD}{\\bo{\\widetilde{D}}}\n",
    "\\newcommand{\\tLa}{\\bo{\\widetilde{\\La}}}\n",
    "\\newcommand{\\tCbe}{\\bo{\\widetilde{C}^{\\be}}}\n",
    "\\newcommand{\\bbe}{\\bo{\\beta}}\n",
    "\\newcommand{\\Cbe}{\\bC^{\\bbe}}\n",
    "\\newcommand{\\bhat}{\\bo{\\hat{\\be}}}\n",
    "\\newcommand{\\pibe}{\\bpi^{\\bbe}}\n",
    "\\newcommand{\\bD}{\\bo{D}}\n",
    "\\newcommand{\\bZ}{\\bo{Z}}\n",
    "\\newcommand{\\bF}{\\bo{F}}\n",
    "\\newcommand{\\bA}{\\bo{A}}\n",
    "\\newcommand{\\bB}{\\bo{B}}\n",
    "\\newcommand{\\bK}{\\bo{K}}\n",
    "\\newcommand{\\bI}{\\bo{I}}\n",
    "\\newcommand{\\bS}{\\bo{S}}\n",
    "\\newcommand{\\bLa}{\\bo{\\La}}\n",
    "\\newcommand{\\ctilde}{\\bo{\\tilde{c}}}\n",
    "\\newcommand{\\bhatK}{\\bo{\\hat{K}}}\n",
    "\\newcommand{\\bR}{\\bo{R}}\n",
    "\\newcommand{\\bAb}{\\bo{\\bar{A}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_init(N):\n",
    "    u = torch.arange(N)\n",
    "    b = torch.sqrt(2 * u[:, None] + 1)\n",
    "    A = b @ b.T\n",
    "    A = torch.tril(A, 0)\n",
    "    A = - (A - torch.diag(u))\n",
    "\n",
    "\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = hippo_init(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_init_dplr(N):\n",
    "    u = torch.arange(N)[:, None]\n",
    "    \n",
    "    p = torch.sqrt(u + 0.5)\n",
    "\n",
    "    # Extract the skew-hermitian part of A.\n",
    "    S = torch.tril(p @ p.T)\n",
    "    S = - S + S.T\n",
    "\n",
    "    # A small trick to make a skew-hermitian matrix a hermitian one.\n",
    "    hermitian_S = S * -1j\n",
    "    Lambda, V = torch.linalg.eigh(hermitian_S)\n",
    "\n",
    "    # Mutliplies back the eigenvalues by (1j)^-1 to retrieve the original eigenvalues of the skew-hermitian matrix.\n",
    "    # We have to add the real parts of the eigenvalues, coming for the 1/2*Id part of the decomposition. \n",
    "    Lambda = Lambda * 1j - 0.5\n",
    "\n",
    "    # Change of basis for b and p.\n",
    "    b = torch.sqrt(2 * u + 1)\n",
    "    b = V.H @ b.type(torch.complex64)\n",
    "    p = V.H @ p.type(torch.complex64)\n",
    "    \n",
    "    return V, Lambda, p, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, Lambda, p, b = hippo_init_dplr(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(V @ (torch.diag(Lambda) - p @ p.H) @ V.H, A.type(torch.complex64), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "The naive way to make the convolution would be the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_conv(A, b, c, L):\n",
    "    return torch.tensor([(c.T @ A.matrix_power(l) @ b).item() for l in range(L)]).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "L = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(N, N)\n",
    "c = torch.randn(N, 1)\n",
    "b = torch.randn(N, 1)\n",
    "u = torch.randn(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = k_conv(A, b, c, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution(u, K):\n",
    "    u_fft = torch.fft.fft(F.pad(u, (0, K.shape[0])))\n",
    "    K_fft = torch.fft.fft(F.pad(K, (0, u.shape[0])))\n",
    "    out = u_fft * K_fft\n",
    "    return torch.fft.ifft(out).real[: u.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5.1633,   3.8721,  35.2096,  62.3083, 227.8797])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_convolution(u, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the main bottleneck of this computation is how we obtain the convolution kernel, because we are exponentiating a matrix L time, compute a lot of matrix products etc, which is highly non efficient.\n",
    "\n",
    "To make this more efficient, we can find more sophisticated algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namely, instead of computing directly $\\bK$, we are going to compute its spectrum, $\\hat{\\bK}$ and then simply apply a inverse Fourier Transform (IDFT). Let $\\om_k = \\exp\\left(-{\\dfrac{2i \\pi k}{N}}\\right)$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\bK}_k &= \\sum_{i=0}^{L-1} \\bK_i \\omega_k^i,\\\\\n",
    "    &= \\sum \\bc^T A^i b \\omega_k^i,\\\\\n",
    "    &= \\bc^T \\left(\\sum \\bA^i \\omega_k^i \\right) \\bb,\\\\\n",
    "    &= \\bc^T (\\bI - \\bA^L)(\\bI - \\bA\\omega_k)^{-1} \\bb.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this does not really reduce the complexity, because now we have to compute an inverse $L$ times (for each $\\om_k$), giving a complexity of $\\mathcal{O}(LN^3)$.\n",
    "\n",
    "\n",
    "However, we are lucky because we are in fact dealing with matrices that have a special form: diagonal + low-rank.\n",
    "\n",
    "Indeed, the matrix $A$ we are using can be decomposed in the following form:\n",
    "\\begin{equation}\n",
    "    \\bA = - \\dfrac{1}{2}\\bI - \\bS - \\bp^*\\bp,\n",
    "\\end{equation}\n",
    "where $\\bS$ is a skew-hermitian matrix ($\\bS = - \\bS^*$), and $\\bp \\in \\RR^{N}$ is a vector of dimension 1.\n",
    "\n",
    "And in the adapted basis, the inverse of a DLPR matrix is much more easy to compute, thanks to the Woodburry inversion formula:\n",
    "\n",
    "\\begin{equation}\n",
    "    (\\bLa - \\bp^* \\bq)^{-1} = \\bLa^{-1} + \\bLa^{-1}\\bp \\left(1 - \\bq^*\\bLa^{-1} \\bp\\right)^{-1}\\bq^* \\bLa^{-1}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this result to the expression of $\\bhatK$, with some more algebraic operations, we obtain a quite simple formula for its expression:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bhatK_k = \\dfrac{2}{1 + \\om_k}\\left[\\ctilde^* \\bR_k \\bb - \\ctilde^* \\bR_k \\bp (1 + \\bp^*\\bR_k \\bp)^{-1}\\bp^*\\bR_k \\bb\\right].\n",
    "\\end{equation}\n",
    "\n",
    "with $\\bR_k = \\left(2\\dfrac{1 - \\om_k}{1 + \\om_k} - \\bLa \\right)^{-1}$ and $\\ctilde^* = \\bc^* (\\bI - \\bAb^L)$.\n",
    "\n",
    "And here, in fact all the matrix multiplications can be evaluated very efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   \\ctilde^T \\bR_k \\bb &= \\sum_{i=1}^{N} \\dfrac{\\ctilde_i \\bb_i}{\\om_k - \\bLa_i},\n",
    "\\end{align}\n",
    "\n",
    "and because we want to compute it for all $\\om_k$, the complexity reduces to $\\mathcal{O}(NL)$.\n",
    "In fact, we can be even more efficient, be we need to rely on algorithms that are not yet implemented on Pytorch.\n",
    "But notice that this complexity is not a bottleneck when the biggest memory cost comes from the size of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy(p, q, lambd, omega_L):\n",
    "    omega_L = 2. * ((1. - omega_L) / (1 + omega_L))\n",
    "    dot_product = p * q\n",
    "    lambd = lambd[:, None]\n",
    "    omega_L = omega_L[None, :]\n",
    "    cauchy_product = dot_product / (omega_L - lambd)\n",
    "    return cauchy_product.sum(axis=-2).view(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_dplr(lambd, b, c, p, L):\n",
    "    omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "    term_1 = cauchy(b, c.conj(), lambd, omega_L)\n",
    "    term_2 = cauchy(c.conj(), p, lambd, omega_L)\n",
    "    term_3 = 1. / (1. + cauchy(p.conj(), p, lambd, omega_L))\n",
    "    term_4 = cauchy(p.conj(), b, lambd, omega_L)\n",
    "    \n",
    "    K_fft = (2. / (1. + omega_L)) * (term_1 - term_2 * term_3 * term_4) \n",
    "    return torch.fft.ifft(K_fft, L).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gen_simple(A, b, c, L):\n",
    "    K = k_conv(A, b, c, L)\n",
    "    def gen(z):\n",
    "        return torch.sum(K *  z ** torch.arange(L))\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gen_inverse(A, b, ct, L):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    A_L = A.matrix_power(L)\n",
    "    return lambda z: (ct.H @ torch.linalg.inv(I - A * z) @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_from_gen(A, b, c, L, inv=False):\n",
    "    if not inv:\n",
    "        gen = k_gen_simple(A, b, c, L)\n",
    "    else:\n",
    "        gen = k_gen_inverse(A, b, c, L)\n",
    "    omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "    at_roots = torch.tensor([gen(z) for z in omega_L])\n",
    "    out = torch.fft.ifft(at_roots, L).reshape(L)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(A, b, c, step=1):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    b_left = torch.linalg.inv(I - (step / 2.0) * A)\n",
    "    A_bar = b_left @ (I + (step / 2.0) * A)\n",
    "    b_bar = (b_left * step) @ b\n",
    "    return A_bar, b_bar, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv_kernel(L=10, N=20):\n",
    "    \n",
    "    V, Lambda, p, b = hippo_init_dplr(N)\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    c = torch.randn(N, 1).type(torch.complex64)\n",
    "    c_basis_diag = V.H @ c.type(torch.complex64)\n",
    "    A_bar, b_bar, c_bar = discretize(A, b, c_basis_diag)\n",
    "    c_tilde_h = c_bar.T @ (torch.eye(N) - A_bar.matrix_power(L))\n",
    "    c_tilde = c_tilde_h.H\n",
    "    K_simple = k_conv(A_bar, b_bar, c_bar, L)\n",
    "    K_efficient = kernel_dplr(Lambda, b, c_tilde, p, L)\n",
    "\n",
    "    assert torch.allclose(K_simple, K_efficient, atol=1e-4)\n",
    "    \n",
    "    u = torch.randn(L)\n",
    "    \n",
    "    y_simple = causal_convolution(u, K_simple)\n",
    "    y_efficient = causal_convolution(u, K_efficient)\n",
    "    assert torch.allclose(y_simple, y_efficient, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multidimensional_input_simple(u, K):\n",
    "    out = torch.zeros_like(u)\n",
    "    for i in range(u.shape[0]):\n",
    "        for j in range(u.shape[-1]):\n",
    "            out[i, :, j] = causal_convolution(u[i, :, j], K)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution_n(u, K):\n",
    "    K = K[None, :, None]\n",
    "    fft_shape = u.shape[1] + K.shape[1]\n",
    "    u_fft = torch.fft.fft(u, n=fft_shape, axis=1)\n",
    "    K_fft = torch.fft.fft(K, n=fft_shape, axis=1)\n",
    "    out = u_fft * K_fft\n",
    "    return torch.fft.ifft(out, axis=1)[:, :u.shape[1], :].real\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multidimensional_convolution(batch_size=8, seq_length=10, hidden_size=5):\n",
    "    u = torch.randn(batch_size, seq_length, hidden_size)\n",
    "    K = torch.randn(seq_length)\n",
    "    simple_conv = multidimensional_input_simple(u, K)\n",
    "    efficient_conv = causal_convolution_n(u, K)\n",
    "    assert torch.allclose(efficient_conv, simple_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multidimensional_convolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent decoding\n",
    "\n",
    "In a situation of unsupervised generation, we need to be able to compute the state space equation autoregressively.\n",
    "But hopefully, thanks to the parametrization of $\\bA$ as a NPLR matrix, we can compute a recurrence step in $\\mathcal{O}(NH)$ where $H$ is the hidden dimension and $N$ the polynomial space dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_dplr(Lambda, p, b, c, L, step=1.):\n",
    "    # Convert parameters to matrices\n",
    "    b = b\n",
    "    ct = c\n",
    "\n",
    "    N = Lambda.shape[0]\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    I = torch.eye(N)\n",
    "\n",
    "    # Forward Euler\n",
    "    A0 = (2.0 / step) * I + A\n",
    "\n",
    "    # Backward Euler\n",
    "    D = torch.diag(1.0 / ((2.0 / step) - Lambda))\n",
    "    \n",
    "    A1 = D - (D @ p * (1.0 / (1 + (p.H @ D @ p))) * p.H @ D)\n",
    "    print(A1.shape)\n",
    "\n",
    "    # A bar and B bar\n",
    "    A_bar = A1 @ A0\n",
    "    b_bar = 2 * A1 @ b\n",
    "\n",
    "    # Recover Cbar from Ct\n",
    "    c_bar = ct.T @ torch.linalg.inv(I - A_bar.matrix_power(L)).conj()\n",
    "    return A_bar, b_bar, c_bar.conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_ssm(Ab, Bb, Cb, u, x0):\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = Ab @ x_k_1 + Bb @ u_k\n",
    "        y_k = Cb @ x_k\n",
    "        return x_k, y_k\n",
    "    recurrence = []\n",
    "    x_k = x0\n",
    "    for i in range(u.shape[0]):\n",
    "        x_k, y_k = step(x_k, u[i])\n",
    "        recurrence.append(y_k)\n",
    "    return x_k, torch.tensor(recurrence).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conversion(N=8, L=16):\n",
    "    V, Lambda, p, b = hippo_init_dplr(N)\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    c = torch.randn(N, 1).type(torch.complex64)\n",
    "    c_basis_diag = V.H @ c.type(torch.complex64)\n",
    "    A_bar, b_bar, c_bar = discretize(A, b, c_basis_diag)\n",
    "    c_tilde_h = c_bar.T @ (torch.eye(N) - A_bar.matrix_power(L))\n",
    "    c_tilde = c_tilde_h.H\n",
    "\n",
    "    # CNN form.\n",
    "    K = kernel_dplr(Lambda, b, c_tilde, p, L)\n",
    "\n",
    "    # RNN form.\n",
    "    Ab, Bb, Cb = discrete_dplr(Lambda, p, b_bar, c_bar, L)\n",
    "    K2 = k_conv(A_bar, b_bar, c_bar, L=L)\n",
    "    assert np.allclose(K, K2, atol=1e-5, rtol=1e-5)\n",
    "\n",
    "    # Apply CNN\n",
    "    u = torch.arange(L).type(torch.complex64)\n",
    "    y1 = causal_convolution(u, K)\n",
    "\n",
    "    # Apply RNN\n",
    "    _, y2 = scan_ssm(\n",
    "        Ab, Bb, Cb, u[:, np.newaxis], torch.zeros((N,)).type(torch.complex64)\n",
    "    )\n",
    "    return y1, y2\n",
    "    assert np.allclose(y1, y2.reshape(-1).real, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "y1, y2 = test_conversion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.7684e-07, -2.4067e-01, -6.8758e-01,  1.8457e-01, -1.8882e-01,\n",
       "         6.2510e-01,  8.6047e-01,  1.7823e+00,  1.9840e+00,  2.8452e+00,\n",
       "         3.2074e+00,  3.8944e+00,  4.4031e+00,  4.9881e+00,  5.5585e+00,\n",
       "         6.1118e+00])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6534, 1.0398, 1.1774, 1.3388, 1.3987, 1.1986, 1.2085, 1.1135,\n",
       "        1.0088, 0.9768, 0.8487, 0.8138, 0.7001, 0.6476, 0.5502])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-s4",
   "language": "python",
   "name": "env-s4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a263f42c39b2fd75b37cb89946a27f6140cd004b337ff228565a862a637fff96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
