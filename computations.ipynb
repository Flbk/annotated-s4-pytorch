{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\CC}{\\mathbb{C}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\Zz}{\\mathcal{Z}}\n",
    "\\newcommand{\\Ww}{\\mathcal{W}}\n",
    "\\newcommand{\\Vv}{\\mathcal{V}}\n",
    "\\newcommand{\\Nn}{\\mathcal{N}}\n",
    "\\newcommand{\\NN}{\\mathcal{N}}\n",
    "\\newcommand{\\Hh}{\\mathcal{H}}\n",
    "\\newcommand{\\Bb}{\\mathcal{B}}\n",
    "\\newcommand{\\Ee}{\\mathcal{E}}\n",
    "\\newcommand{\\Cc}{\\mathcal{C}}\n",
    "\\newcommand{\\Gg}{\\mathcal{G}}\n",
    "\\newcommand{\\Ss}{\\mathcal{S}}\n",
    "\\newcommand{\\Pp}{\\mathcal{P}}\n",
    "\\newcommand{\\Ff}{\\mathcal{F}}\n",
    "\\newcommand{\\Xx}{\\mathcal{X}}\n",
    "\\newcommand{\\Mm}{\\mathcal{M}}\n",
    "\\newcommand{\\Ii}{\\mathcal{I}}\n",
    "\\newcommand{\\Dd}{\\mathcal{D}}\n",
    "\\newcommand{\\Ll}{\\mathcal{L}}\n",
    "\\newcommand{\\Tt}{\\mathcal{T}}\n",
    "\\newcommand{\\al}{\\alpha}\n",
    "\\newcommand{\\la}{\\lambda}\n",
    "\\newcommand{\\ga}{\\gamma}\n",
    "\\newcommand{\\Ga}{\\Gamma}\n",
    "\\newcommand{\\La}{\\Lambda}\n",
    "\\newcommand{\\si}{\\sigma}\n",
    "\\newcommand{\\Si}{\\Sigma}\n",
    "\\newcommand{\\be}{\\beta}\n",
    "\\newcommand{\\de}{\\delta}\n",
    "\\newcommand{\\De}{\\Delta}\n",
    "\\renewcommand{\\phi}{\\varphi}\n",
    "\\renewcommand{\\th}{\\theta}\n",
    "\\newcommand{\\om}{\\omega}\n",
    "\\newcommand{\\Om}{\\Omega}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\newcommand{\\bo}[1]{\\boldsymbol{\\mathbf{#1}}}\n",
    "\\newcommand{\\bu}{\\bo{u}}\n",
    "\\newcommand{\\bv}{\\bo{v}}\n",
    "\\newcommand{\\bV}{\\bo{V}}\n",
    "\\newcommand{\\bC}{\\bo{C}}\n",
    "\\newcommand{\\bp}{\\bo{p}}\n",
    "\\newcommand{\\bq}{\\bo{q}}\n",
    "\\newcommand{\\bX}{\\bo{X}}\n",
    "\\newcommand{\\bc}{\\bo{c}}\n",
    "\\newcommand{\\bb}{\\bo{b}}\n",
    "\\newcommand{\\bh}{\\bo{h}}\n",
    "\\newcommand{\\by}{\\bo{y}}\n",
    "\\newcommand{\\lc}{\\bo{L}(\\bC)}\n",
    "\\newcommand{\\lcb}[1]{\\bo{L}(\\bo{#1})}\n",
    "\\newcommand{\\ba}{\\bo{a}}\n",
    "\\newcommand{\\bbv}{\\bo{b}}\n",
    "\\newcommand{\\tD}{\\bo{\\widetilde{D}}}\n",
    "\\newcommand{\\tLa}{\\bo{\\widetilde{\\La}}}\n",
    "\\newcommand{\\tCbe}{\\bo{\\widetilde{C}^{\\be}}}\n",
    "\\newcommand{\\bbe}{\\bo{\\beta}}\n",
    "\\newcommand{\\Cbe}{\\bC^{\\bbe}}\n",
    "\\newcommand{\\bhat}{\\bo{\\hat{\\be}}}\n",
    "\\newcommand{\\pibe}{\\bpi^{\\bbe}}\n",
    "\\newcommand{\\bD}{\\bo{D}}\n",
    "\\newcommand{\\bZ}{\\bo{Z}}\n",
    "\\newcommand{\\bF}{\\bo{F}}\n",
    "\\newcommand{\\bA}{\\bo{A}}\n",
    "\\newcommand{\\bB}{\\bo{B}}\n",
    "\\newcommand{\\bK}{\\bo{K}}\n",
    "\\newcommand{\\bI}{\\bo{I}}\n",
    "\\newcommand{\\bS}{\\bo{S}}\n",
    "\\newcommand{\\bLa}{\\bo{\\La}}\n",
    "\\newcommand{\\ctilde}{\\bo{\\tilde{c}}}\n",
    "\\newcommand{\\ptilde}{\\bo{\\tilde{p}}}\n",
    "\\newcommand{\\bhatK}{\\bo{\\hat{K}}}\n",
    "\\newcommand{\\bR}{\\bo{R}}\n",
    "\\newcommand{\\bAb}{\\bo{\\bar{A}}}\n",
    "\\newcommand{\\bbb}{\\bo{\\bar{b}}}\n",
    "\\newcommand{\\ut}{u_{<t}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|}\n",
    "\\newcommand{\\clint}[1]{\\left[ #1 \\right]}\n",
    "\\newcommand{\\diff}{\\mathop{}\\!\\mathrm{d}}\n",
    "\\newcommand{\\dotp}[1]{\\left\\langle #1 \\right\\rangle}\n",
    "\\newcommand{\\bx}{\\bo{x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 in Pytorch.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook aims at explaining the S4 layer described in [S4](https://arxiv.org/abs/2111.00396), with more details on practical use cases, as well as a simple Pytorch implementation.\n",
    "Some parts of this code are directly coming from the excellent guide [Annotated S4](https://srush.github.io/annotated-s4/).\n",
    "\n",
    "The goal of the notebook is to be as concise as possible while providing enough details for a full comprehension of a practical implementation of S4 in Pytorch.\n",
    "\n",
    "# Incoming\n",
    "\n",
    "Note that the goal of this notebook is illustrative. We are soon going to release more efficient codes (in full Pytorch) that can handle real sized data. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelisation\n",
    "\n",
    "## Classical approaches\n",
    "\n",
    "Deep NLP has made huge progress since the arrival of the sequence-to-sequence models. A sequence-to-sequence model is a model that maps a sequence $u_0, \\dots, u_{L-1}$ to another sequence $y_0, \\dots, y_{L-1}$.\n",
    "\n",
    "The main approaches in NLP were first RNN, that mimics Hidden Markov Process. In simple terms, a RNN models a situation recursively:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        h_{k+1} &=& g(u_{k+1}, h_k),\\\\\n",
    "        y_{k+1} &=& f(h_{k+1}).\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "where $h_k$ represents a hidden state. Here each output $y_k$ is built recursively on the precedent value $h_{k-1}$. The main drawbacks of such models is that it is extremely difficult control how the information of previous steps flows up to the current step. It has even been derived that statistically such networks doesn't have long-memory (i.e the output at step $k$ is unlikely to have retained information of $u_{k-l}$if $l$ is large enough).\n",
    "\n",
    "The other and currently SOTA architecture is the transformer.\n",
    "\n",
    "Transformers are extremelly powerful because the output sequence depends directly on *all* the previous steps:\n",
    "\n",
    "\\begin{equation}\n",
    "    y_{k+1} = \\sum_{i=0}^{k} f(u_i, u_{k+1}).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore they do have long-memory. But here the problem comes form the memory consumption of such operation. To process a full sequence the memory complexity is $\\mathcal{O}(L^2)$.\n",
    "\n",
    "## State-space approach\n",
    "\n",
    "To overcome these problems, S4 introduces the formalism of state-space models. Here $u_k \\in \\RR$ is a scalar input.\n",
    "\n",
    "\\begin{equation}\\label{eq:state-space}\n",
    "    \\begin{cases}\n",
    "        x_{k+1} = \\bA x_k + \\bb u_{k+1},\\\\\n",
    "        y_{k+1} = \\bc^T x_{k+1}.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "where $x_k \\in \\RR^N$ is a vector and obviously $\\bc \\in \\RR^N$. For multidimensionnal inputs, we simply apply these steps (with the same parameters) independently to all coordinates.\n",
    "\n",
    "Let's write a first implementation of such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive SSM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSSM:\n",
    "    \"\"\"Abstract base class for state-space model in Python.\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, c):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSSM(BaseSSM):\n",
    "    \"\"\"Simple state space model with recurrent computations.\"\"\"\n",
    "\n",
    "    def __call__(self, u, L):\n",
    "        \"\"\"x_k = A x{k-1} + b u_{k+1}\"\"\"\n",
    "        y = torch.zeros(L)\n",
    "        x_k = torch.zeros((self.A.shape[0], 1))\n",
    "        for i in range(L):\n",
    "            x_k = self.A @ x_k + b * u[i]\n",
    "\n",
    "            # Compute the output y[k]\n",
    "            y_k = self.c.T @ x_k\n",
    "            y[i] = y_k\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "L = 5\n",
    "\n",
    "A = torch.randn(N, N)\n",
    "b = torch.rand(N, 1)\n",
    "c = torch.rand(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ssm = SimpleSSM(A, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f18625b18d0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_interval = torch.arange(L)\n",
    "u = torch.sin(2 * torch.pi * torch.arange(L) / L)\n",
    "plt.plot(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f18602e1a20>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = simple_ssm(u, L)\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-space computations\n",
    "\n",
    "Now let's take a step back and remember that our goal is to train a model, whose parameters will include $\\bA, \\bb, \\bc$.\n",
    "\n",
    "However if the formulation of \\eqref{eq:state-space} seems simple, it is in fact quite complicated for training, due do its recursive definition (same problems faced during training of RNN actually).\n",
    "\n",
    "A big advantage of transformers is the possibility to process in parallel the whole sequence, that greatly improves the training speed and the backpropagation of the gradient is more easy than with recursive architectures.\n",
    "\n",
    "Hence a legitimate question is: do we have the same property with S4? And the answer is yes!\n",
    "\n",
    "And for autoregressive inference, we can simply apply the recurrent expression.\n",
    "\n",
    "\n",
    "## Training - Convolution view\n",
    "\n",
    "For the convolution view, we are now going to focus exclusively on training, i.e when the full input and output are already available.\n",
    "\n",
    "One can notice that if we unrol \\eqref{eq:state-space}, $y \\in \\RR^L$ can be directly expressed as a convolution between the input $u \\in \\RR^L$ and a filter $\\bK \\in \\RR^L$:\n",
    "\n",
    "\\begin{align}\n",
    "    y_0 &= \\bc^T\\bb u_0,\\\\\n",
    "    y_1 &= \\bc^T\\bA\\bb u_0 + \\bc^T\\bb u_1,\\\\\n",
    "    y_2 &= \\bc^T \\bA^2 \\bb u_0 + \\bc^T\\bA\\bb u_1 + \\bc^T\\bb u_2,\\\\\n",
    "    \\vdots\\\\\n",
    "    y_k &= \\sum_{i=0}^k \\bc^T\\bA^i\\bb u_{k - i}.\n",
    "\\end{align}\n",
    "\n",
    "We have therefore: $y = \\bK * u$, with $\\bK = \\left(\\bc^T \\bb, \\bc^T\\bA\\bb, \\dots, \\bc^T\\bA^{L-1}\\bb\\right)$.\n",
    "\n",
    "## Inference - Recurrent decoding\n",
    "\n",
    "At inference we cannot always use the same computation, especially in the autoregressive setup. In this case we have to get back to the naive recurrent definition of an SSM. However, we will see later that it is possible to make it more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations and details on the convolution view\n",
    "\n",
    "In this section we are going to provide fully working and tested Pytorch implementation of the convolution formulation. Remember that here the full input is available.\n",
    "\n",
    "Let's begin with a good news: a convolution can be computed in $\\mathcal{O}(L \\log L)$ once the filter $\\bK$ is known, using Discrete Fourier Transform.\n",
    "\n",
    "Compared to the quadratic complexity of transformers, we have a huge gain in term of memory usage.\n",
    "\n",
    "Let's write a first function to see what it gives.\n",
    "\n",
    "*Remark:* In the following code you will see some functions characteristic of hermitian product in a complex vector space. This is because later we will work in $\\CC^N$ instead of $\\RR^N$. For now you can ignore it and just think of `c.H` as `c.T` for the adjoint operator and ignore `c.conj()` (that we use sometimes to compute the adjoint operator indirectly).\n",
    "\n",
    "But sometimes some operations are also needed because of floating points error (like taking the real part after inverse Fourier Transform). When there is ambiguity we will write some comments in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution with DFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminders on convolutions\n",
    "Before going into the implementations, just a quick recap on discrete convolutions with Fourier Transform.\n",
    "\n",
    "Let $u \\in \\RR^{L}$ and $v \\in \\RR^{L}$. We consider the \"periodized\" version of $v$, $\\tilde{v}$ such that $\\tilde{v}: \\ZZ \\to \\RR$ and $\\forall n \\in \\ZZ, \\ \\tilde{v}[n] = v[n \\mod L]$.\n",
    "\n",
    "The discrete *circular* convolution between $u$ and $v$ is defined as:\n",
    "\\begin{equation}\\label{eq:circular-conv}\n",
    "    \\forall k \\in [0..L-1], \\ (u \\circledast v)[k] = \\sum_{i=0}^{L-1} u[i] \\tilde{v}[k - i].\n",
    "\\end{equation}\n",
    "\n",
    "And the convolution theorem states that (the derivation consists only on permuting the $\\sum$ symbols):\n",
    "\n",
    "\\begin{equation}\n",
    "    u \\circledast v = \\mathcal{F}^{-1} \\left(\\hat{u} \\odot \\hat{v}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{.}$ designates the DFT of a sequence and $\\odot$ designates the elementwise multiplication (we consider indeed $\\hat{u}$ and the others sequences as vectors of $\\RR^L$.).\n",
    "\n",
    "### The necessity of zero-padding\n",
    "\n",
    "Ok, now that we know how to compute the *circular* convolution, how do we compute something like we have earlier, i.e:\n",
    "\n",
    "$(u * v)[k] = \\sum_{i=0}^{k} u[i]v[k - i]$ ?\n",
    "\n",
    "A very famous trick is simply to pad $u$ and $v$ with $L$ zeros *before* taking their DFT. Indeed, replace $u$ and $v$ with their padded version (hence vectos of $\\RR^{2L}$) in \\eqref{eq:circular-convolution} and you'll see immediately that it coincides with the *non-circular* convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution(u, K):\n",
    "    convolution_shape = u.shape[0] + K.shape[0]\n",
    "\n",
    "    # Specifying the paramter 'n' will automatically pad our vectors to the right dimension.\n",
    "    u_fft = torch.fft.fft(u, n=convolution_shape)\n",
    "    K_fft = torch.fft.fft(K, n=convolution_shape)\n",
    "    out = u_fft * K_fft\n",
    "\n",
    "    # This will give complex outputs.\n",
    "    return torch.fft.ifft(out)[: u.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive kernel computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_simple(A, b, c, L):\n",
    "    \"\"\"Compute K naively with its explicit expression.\"\"\"\n",
    "    K = [(c.H @ A.matrix_power(l) @ b).item() for l in range(L)]\n",
    "    return torch.tensor(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvSSM(BaseSSM):\n",
    "    def __call__(self, u, L):\n",
    "\n",
    "        K = kernel_simple(self.A, self.b, self.c, L)\n",
    "        y = causal_convolution(u, K)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_conv_ssm = SimpleConvSSM(A, b, c)\n",
    "y_conv = simple_conv_ssm(u, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    y.type(torch.complex64), y_conv, atol=1e-3\n",
    ")  # We need to cast y to complex type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient formulation and kernel computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the main bottleneck of this computation is how we obtain the convolution kernel. We exponentiate a matrix $L$ times, compute a lot of matrix products etc, which is highly non efficient and non stable from a numerical point of view.\n",
    "\n",
    "To make this more efficient, we can find more sophisticated algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namely, instead of computing directly $\\bK$, we are going to compute its DFT, $\\bhatK$ and then simply apply a inverse Fourier Transform (IDFT). Let $\\om_k = \\exp\\left(-{\\dfrac{2i \\pi k}{N}}\\right)$.\n",
    "\n",
    "\n",
    "\n",
    "*Remark:* At this point, we are going to use exclusively complex numbers and hence work in a complex Hilbert space. The main notable difference is that the adjoint operator will now be *conjugation + transpose* instead of simply *transpose*.\n",
    "\n",
    "This applies especially to $\\bc$.\n",
    "\\begin{align}\\label{eq:K-spectrum}\n",
    "    \\bhatK_k &= \\sum_{i=0}^{L-1} \\bK_i \\omega_k^i,\\\\\n",
    "    &= \\sum \\bc^* A^i b \\omega_k^i,\\\\\n",
    "    &= \\bc^* \\left(\\sum \\bA^i \\omega_k^i \\right) \\bb,\\\\\n",
    "    &= \\bc^* (\\bI - \\bA^L)(\\bI - \\bA\\omega_k)^{-1} \\bb.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this does not really reduce the complexity, because now we have to compute an inverse $L$ times (for each $\\om_k$), giving a complexity of $\\mathcal{O}(LN^3)$.\n",
    "\n",
    "To see how we can improve it, let's give more details about the forms of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and special decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization\n",
    "Instead of directly computing the state-space recurrence with the matrices $\\bA$ and $\\bb$ we are going to use slightly modified versions $\\bAb, \\bbb$. The rationale for doing so will be explained later.\n",
    "\n",
    "For now, one can assume that instead of using directly $\\bA$ and $\\bb$ directly we are going to use:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\bAb &= \\left(\\bI - \\dfrac{1}{2}\\bA \\right)^{-1}\\left(\\bI + \\dfrac{1}{2}\\bA\\right),\\\\\n",
    "        \\bbb &= \\left(\\bI - \\dfrac{1}{2}\\bA \\right)^{-1}\\bb.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Theses versions are called *discretized* (explanations given later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(A, b, step=1):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    left_term = torch.linalg.inv(I - (step / 2.0) * A)\n",
    "    A_bar = left_term @ (I + (step / 2.0) * A)\n",
    "    b_bar = (left_term * step) @ b\n",
    "    return A_bar, b_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decomposition\n",
    "\n",
    "Moreover, we are going to assume that $\\bA$ is similar to a matrix of the form $\\bLa - \\bp^* \\bp$, where we now considerate matrices and vectors with coefficients in $\\CC$.\n",
    "Where $\\bLa \\in \\CC^{N \\times N}$ is diagonal and $\\bp \\in \\CC^{N \\times 1}$.\n",
    "\n",
    "$.^*$ designated the hermitian adjoint (= transpose + conjugate) of a matrix (a vector is viewed as a matrix of $\\CC^{N \\times 1}$).\n",
    "This decomposition is called **Diagonal Plus Low Rank (DPLR)**.\n",
    "\n",
    "*Remark:* In the original article the derivations are made with $\\bA \\sim \\bLa + \\bq^* \\bp$. But in practice in S4 we use $-\\bp = \\bq$ which is valid and works better for numerical reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And in the adapted basis, the inverse of a DLPR matrix is much more easy to compute, thanks to the Woodburry inversion formula:\n",
    "\n",
    "\\begin{equation}\n",
    "    (\\bLa - \\bp^* \\bq)^{-1} = \\bLa^{-1} + \\bLa^{-1}\\bp \\left(1 - \\bq^*\\bLa^{-1} \\bp\\right)^{-1}\\bq^* \\bLa^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "One can notice that this inverse requires only $\\mathcal{O}(N)$ operations (each term can be computed in linear time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this result to the expression of $\\bhatK$ \\eqref{eq:K-spectrum} using $\\bAb$ and $\\bbb$, with some more algebraic manipulations (details in the original article), we obtain a quite simple formula for its expression:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bhatK_k = \\dfrac{2}{1 + \\om_k}\\left[\\ctilde^* \\bR_k \\bb - \\ctilde^* \\bR_k \\bp (1 + \\bp^*\\bR_k \\bp)^{-1}\\bp^*\\bR_k \\bb\\right].\n",
    "\\end{equation}\n",
    "\n",
    "with $\\bR_k = \\left(2\\dfrac{1 - \\om_k}{1 + \\om_k} - \\bLa \\right)^{-1}$ and $\\ctilde^* = \\bc^* (\\bI - \\bAb^L)$.\n",
    "\n",
    "And here, in fact all the matrix multiplications can be evaluated very efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\\label{eq:cauchy-K}\n",
    "   \\ctilde^* \\bR_k \\bb &= \\sum_{i=1}^{N} \\dfrac{\\ctilde_i \\bb_i}{\\om_k - \\bLa_i},\n",
    "\\end{align}\n",
    "\n",
    "and because we want to compute it for all $\\om_k$, the complexity reduces to $\\mathcal{O}(NL)$.\n",
    "In fact, we can be even more efficient, but we need to rely on algorithms that are not yet implemented on Pytorch.\n",
    "But notice that evaluating this product in $\\mathcal{O}(NL)$ is not a bottleneck (if implementing it efficiently) as in practical applications we are often limited by the sequence length size.\n",
    "\n",
    "Computing operation \\eqref{eq:cauchy-K} for all $k$ is actually a Cauchy product, a well studied problem in the litterature. We are going to implement it in a naive way, but again it won't be the main bottleneck of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Efficient convolution implementation\n",
    "We have to implement at least two functions:\n",
    "\n",
    "- `cauchy` that will compute the terms like $\\ctilde^* \\bR_k \\bb$. It takes 3 parameters as input for the vectors $(\\ctilde, \\bp, \\bLa)$ and another `omega_L` for comuting $\\bR_k$ for each $0 \\leq k \\leq L - 1$.\n",
    "- `kernel_dplr` that will compute the $\\bhatK$ using `cauchy`.\n",
    "- `compute_c_tilde` to compute $\\ctilde$ given the original $\\bc$ (this function won't be useful in the final S4 implementation but we will need it for testing purposes, i.e to check that $\\bK$ obtained with our hard computations work indeed yields the same result than a naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_c_tilde(A, c, L):\n",
    "    I = torch.eye(A.shape[0])\n",
    "\n",
    "    # .H returns the adjoint operator (equivalent to transpose with real inputs).\n",
    "    return (I - A.matrix_power(L)).H @ c\n",
    "\n",
    "\n",
    "def cauchy(p, q, Lambda, omega_L):\n",
    "    omega_L = 2.0 * ((1.0 - omega_L) / (1 + omega_L))\n",
    "\n",
    "    dot_product = p.conj() * q\n",
    "\n",
    "    Lambda = Lambda[:, None]\n",
    "    omega_L = omega_L[None, :]\n",
    "    cauchy_product = dot_product / (omega_L - Lambda)\n",
    "    return cauchy_product.sum(axis=-2)\n",
    "\n",
    "\n",
    "def kernel_dplr(Lambda, p, b, c_tilde, L):\n",
    "    omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "    term_1 = cauchy(c_tilde, b, Lambda, omega_L)\n",
    "    term_2 = cauchy(c_tilde, p, Lambda, omega_L)\n",
    "    term_3 = 1.0 / (1.0 + cauchy(p, p, Lambda, omega_L))\n",
    "    term_4 = cauchy(p, b, Lambda, omega_L)\n",
    "\n",
    "    K_fft = (2.0 / (1.0 + omega_L)) * (term_1 - term_2 * term_3 * term_4)\n",
    "    return torch.fft.ifft(K_fft, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dplr_ssm_random(N):\n",
    "    # Create a random DPLR decomposition.\n",
    "\n",
    "    Lambda = -torch.rand(\n",
    "        N, dtype=torch.complex64\n",
    "    )  # use negative real parts to get more stability\n",
    "    p = torch.randn(N, 1, dtype=torch.complex64)\n",
    "    b = torch.randn(N, 1, dtype=torch.complex64)\n",
    "    c = torch.randn(N, 1, dtype=torch.complex64)\n",
    "\n",
    "    return Lambda, p, b, c\n",
    "\n",
    "\n",
    "Lambda, p, b, c = make_dplr_ssm_random(N)\n",
    "\n",
    "\n",
    "A = torch.diag(Lambda) - p @ p.H\n",
    "A_bar, b_bar = discretize(A, b)\n",
    "c_tilde = compute_c_tilde(A_bar, c, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the computations of the kernel yields the same result than with a naive approach.\n",
    "K_dplr = kernel_dplr(Lambda, p, b, c_tilde, L)\n",
    "K_simple = kernel_simple(A_bar, b_bar, c, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(K_simple, K_dplr, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the convolutional SSM quite efficiently and in a very stable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientConvSSM(BaseSSM):\n",
    "    def __init__(self, Lambd, p, b, c):\n",
    "        self.Lambda = Lambda\n",
    "        self.p = p\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, u, L):\n",
    "        A = torch.diag(self.Lambda) - p @ p.H\n",
    "        A_bar, _ = discretize(A, self.b)\n",
    "        c_tilde = compute_c_tilde(A_bar, c, L)\n",
    "        K = kernel_dplr(self.Lambda, p, b, c_tilde, L)\n",
    "        y = causal_convolution(u, K)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares efficient implentation with the naive one.\n",
    "efficient_conv_ssm = EfficientConvSSM(Lambda, p, b, c)\n",
    "y_efficient_conv = efficient_conv_ssm(u, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_conv_ssm = SimpleConvSSM(A_bar, b_bar, c)\n",
    "y_simple_conv = simple_conv_ssm(u, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(y_efficient_conv, y_simple_conv, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multidimensional data\n",
    "\n",
    "One can have noticed that until now we have only used 1D sequences. In real applications we always need to use multi-dimensional data.\n",
    "As said earlier, a direct way to generalize everything is to apply all this processing to all dimensions independently.\n",
    "\n",
    "To do that in Pytorch, we are going to rely a lot on broadcasting. The drawback is that everything will be less easily readible.\n",
    "\n",
    "A good way to keep track of what is happening is to always remind the shape of the input tensors.\n",
    "\n",
    "\n",
    "And in addition we are also going to consider a batch dimension.\n",
    "\n",
    "In what will follow:\n",
    "- `L` will be sequence length.\n",
    "- `N` will be the state dimension.\n",
    "- `H` will be the embedding dimension (instead of having $u \\in \\RR^L$, we have $u \\in \\RR^{H\\times L}$.\n",
    "- `B` will be the batch size (finally $u \\in \\RR^{B \\times H\\times L}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi dimenstional discretization\n",
    "Let's first vectorize the discretization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_multi(A, b, step=1):\n",
    "    # A (H, N, N)\n",
    "    # b (H, N)\n",
    "    I = torch.eye(A.shape[1])\n",
    "    left_term = torch.linalg.inv(I[None, :, :] - (step / 2.0) * A)\n",
    "    A_bar = left_term @ (I + (step / 2.0) * A)\n",
    "    b_bar = (left_term * step) @ b[:, :, None]\n",
    "    return A_bar, b_bar.reshape(b.shape[0], -1)\n",
    "\n",
    "\n",
    "def discretize_multi_simple(A, b, step=1):\n",
    "    # A (H, N, N)\n",
    "    # b (H, N)\n",
    "    A_bar = torch.zeros_like(A)\n",
    "    b_bar = torch.zeros_like(b)\n",
    "    for h in range(A.shape[0]):\n",
    "        A_bar[h], b_bar[h] = discretize(A[h], b[h], step=step)\n",
    "    return A_bar, b_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3\n",
    "A = torch.randn(H, N, N)\n",
    "b = torch.randn(H, N)\n",
    "c = torch.randn(H, N)\n",
    "A_bar, b_bar = discretize_multi(A, b)\n",
    "A_bar_simple, b_bar_simple = discretize_multi_simple(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_bar, b_bar = discretize_multi(A, b)\n",
    "A_bar_simple, b_bar_simple = discretize_multi_simple(A, b)\n",
    "\n",
    "assert torch.allclose(A_bar_simple, A_bar, atol=1e-4)\n",
    "assert torch.allclose(b_bar, b_bar_simple, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a utility function to help us \"vectorize\" our unidimensionnal functions for testing (it just does a for loop but it will help us avoid repeating code).\n",
    "\n",
    "This roughly does the same than the manual implementation of `discretize_multi_simple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi(func):\n",
    "    \"\"\"Makes a function handle multi dimensionnal inputs.\n",
    "    This is not optimized and should only be used for testing purposes.\"\"\"\n",
    "\n",
    "    def multi_inputs(*args):\n",
    "        out = []\n",
    "        hidden_dim = args[0].shape[0]\n",
    "\n",
    "        args = list(args)\n",
    "        for i in range(len(args)):\n",
    "            if not isinstance(args[i], torch.Tensor):\n",
    "\n",
    "                args[i] = [args[i] for _ in range(hidden_dim)]\n",
    "\n",
    "        for h in range(hidden_dim):\n",
    "            out.append(func(*[input_tensor[h] for input_tensor in args]).tolist())\n",
    "        return torch.tensor(out)\n",
    "\n",
    "    return multi_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi dimensional kernel computation\n",
    "Now the kernel generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_c_tilde_multi(A, c, L):\n",
    "    # A (H, N, N)\n",
    "    # c (H, N)\n",
    "    c_tilde = (torch.eye(A.shape[1])[None, :, :] - A.matrix_power(L)).mH @ c[:, :, None]\n",
    "    return c_tilde.view(c_tilde.shape[0], -1)\n",
    "\n",
    "\n",
    "def cauchy_multi(p, q, Lambd, omega_L):\n",
    "    \"\"\"Multidimensionnal Cauchy product. Basically we are just using broadcasting to generalize above function.\"\"\"\n",
    "    # p, q, Lambd (H, N)\n",
    "    # omega_L (L,)\n",
    "    omega_L = 2.0 * ((1.0 - omega_L) / (1 + omega_L))\n",
    "    dot_product = p.conj() * q\n",
    "\n",
    "    Lambd = Lambd[:, :, None]\n",
    "    omega_L = omega_L[None, None, :]\n",
    "    cauchy_product = dot_product[:, :, None] / (omega_L - Lambd)\n",
    "    return cauchy_product.sum(axis=-2)\n",
    "\n",
    "\n",
    "def kernel_dplr_multi(Lambd, p, b, c_tilde, L):\n",
    "    # c_tilde, p, q, Lambd (H, N)\n",
    "    omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "    term_1 = cauchy_multi(c_tilde, b, Lambd, omega_L)\n",
    "    term_2 = cauchy_multi(c_tilde, p, Lambd, omega_L)\n",
    "    term_3 = 1.0 / (1.0 + cauchy_multi(p, p, Lambd, omega_L))\n",
    "    term_4 = cauchy_multi(p, b, Lambd, omega_L)\n",
    "\n",
    "    K_fft = (2.0 / (1.0 + omega_L[None, :])) * (term_1 - term_2 * term_3 * term_4)\n",
    "    return torch.fft.ifft(K_fft, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tilde = compute_c_tilde_multi(A, c, L)\n",
    "\n",
    "compute_c_tilde_multi_simple = make_multi(compute_c_tilde)\n",
    "c_tilde_simple = compute_c_tilde_multi_simple(A, c, L)\n",
    "\n",
    "# Check if our vectorized function gives the same result than the a naive one.\n",
    "assert torch.allclose(c_tilde, c_tilde_simple, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dplr_ssm_random_multi(H, N):\n",
    "    Lambda = torch.randn(H, N, dtype=torch.complex64)\n",
    "    p = torch.randn(H, N, dtype=torch.complex64)\n",
    "    b = torch.randn(H, N, dtype=torch.complex64)\n",
    "    c = torch.randn(H, N, dtype=torch.complex64)\n",
    "\n",
    "    return Lambda, p, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with complex inputs, just to make sure we will be able to generalize with these kind of inputs.\n",
    "Lambda, p, b, c = make_dplr_ssm_random_multi(H, N)\n",
    "omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L)\n",
    "\n",
    "prod = cauchy_multi(p, b, Lambda, omega_L)\n",
    "\n",
    "cauchy_multi_simple = make_multi(cauchy)\n",
    "prod_simple = cauchy_multi_simple(\n",
    "    p[:, :, None], b[:, :, None], Lambda, omega_L.expand(H, L)\n",
    ")\n",
    "\n",
    "assert torch.allclose(prod_simple, prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = kernel_dplr_multi(Lambda, p, b, c, L)\n",
    "\n",
    "kernel_dplr_multi_simple = make_multi(kernel_dplr)\n",
    "\n",
    "K_simple = kernel_dplr_multi_simple(\n",
    "    Lambda, p[:, :, None], b[:, :, None], c[:, :, None], L\n",
    ")\n",
    "\n",
    "# Compare the kernels with our vectorized and naive approaches.\n",
    "assert torch.allclose(K, K_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi dimensional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution_multi(u, K):\n",
    "    # u (B, H, L)\n",
    "    # K (H, L)\n",
    "    fft_shape = u.shape[2] + K.shape[1]\n",
    "    u_fft = torch.fft.fft(u, n=fft_shape, axis=-1)\n",
    "    K_fft = torch.fft.fft(K, n=fft_shape, axis=-1)\n",
    "    out = u_fft * K_fft\n",
    "    return torch.fft.ifft(out, axis=-1)[:, :, : u.shape[2]]\n",
    "\n",
    "\n",
    "def causal_convolution_multi_simple(u, K):\n",
    "    # u (B, H, L)\n",
    "    # K (H, L)\n",
    "    out = torch.zeros_like(u, dtype=torch.complex64)\n",
    "    for i in range(u.shape[0]):\n",
    "        for j in range(u.shape[1]):\n",
    "            out[i, j, :] = causal_convolution(u[i, j, :], K[j])\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_convolution_multi(batch_size=8, seq_length=10, hidden_size=5):\n",
    "    \"\"\"Function to test if the multidimensionnal convolution works.\"\"\"\n",
    "    u = torch.randn(batch_size, hidden_size, seq_length)\n",
    "    K = torch.randn(hidden_size, seq_length)\n",
    "\n",
    "    y_simple = causal_convolution_multi_simple(u, K)\n",
    "    y = causal_convolution_multi(u, K)\n",
    "    assert torch.allclose(y, y_simple, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_convolution_multi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok it was a bit tedious but we did it.\n",
    "\n",
    "Now we are ready to implement everything in a `nn.Module` class.\n",
    "We have indeed a desribed a valid model whose parameters are $\\bLa, \\bb, \\bp, \\bc$.\n",
    "\n",
    "There is however a last detail on which we should spend some time. Indeed, we are going to initialize $\\bLa, \\bb, \\bp$ with very special values. This is also this initializatin which is going to explain why this S4 model works so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPPO matrix\n",
    "\n",
    "First a bit of context. In the previous section we have introduced a discretized version as well as DPLR decomposition for $\\bA$. But the main contribution of the article is to provide a value of $\\bA$ that will efficiently keep in memory the information on the whole sequence.\n",
    "\n",
    "To do that, let's go back in the continuous domain and consider $u(t)$, $u \\colon \\RR^+ \\to \\RR$ a scalar continuous function. Because we are in the end interesting in autoregressive generation, let's introduce $\\ut = u \\mathbf{1}_{\\clint{0, t}}$. And what the authors propose is to approximate this function $\\ut$ on a polynomial basis, *for each* $t \\in \\RR^+$.\n",
    "\n",
    "For time $t$ we choose an orthogonal polynomial basis $(P_n^t)_{0\\leq n \\leq N-1}\\in \\RR_N[X]$ for the scalar product $\\dotp{u, v} = \\displaystyle\\int_0^t u v \\diff \\lambda$ (typically we orthogonalize the canonical basis).\n",
    "\n",
    "$\\forall n \\in [0..N-1]$ the coefficients $x_n(t)$ of the projection are simply given by:\n",
    "\n",
    "\\begin{equation}\\label{eq:coefs}\n",
    "    x_n(t) = \\displaystyle\\int_0^t \\ut(s) P^t_n(s) \\diff s.\n",
    "\\end{equation}\n",
    "\n",
    "When we differentiate \\eqref{eq:coefs} (see the original paper for the details), we find something that again depends on the $x_1, \\dots, x_N$ and $\\ut$, hence we have an ODE of the form:\n",
    "\n",
    "\\begin{equation}\\label{eq:ode}\n",
    "    \\dfrac{d \\bx}{dt}(t) = \\bA \\bx(t) + u(t)\\bb,\n",
    "\\end{equation}\n",
    "with $\\bA \\in \\RR^{N \\times N}$ and $\\bb \\in \\RR^N$.\n",
    "\n",
    "At each time $t$, $\\bx(t) \\in \\RR^N$ has therefore the powerful property to represent the whole function $\\ut$. Hence, the memory property.\n",
    "And it happens that for the Legend polynomial basis, the matrices $\\bA$ and $\\bb$ have a closed form expression:\n",
    "\n",
    "\\begin{array}{rrrl}\n",
    "    \\forall i, j \\in [0..N-1], & \\, \\bA_{ij} & = & -\n",
    "    \\begin{cases}\n",
    "        (2i + 1)^{1/2}(2j + 1)^{1/2} &\\quad \\text{if} \\, i > j,\\\\\n",
    "        i + 1 &\\quad \\text{if} \\, i = j,\\\\\n",
    "        0 &\\quad \\text{if} \\, i < j,\n",
    "    \\end{cases}\\\\\n",
    "    \\forall i \\in [0..N-1], & \\, \\bb_i & = & (2i + 1)^{1/2}.\n",
    "\\end{array}\n",
    "## Discretization\n",
    "\n",
    "Up to now we have been using continuous input, but we are in the end interested in *discrete* inputs. To adapt everything, the authors simply proposed to discretize the ODE.\n",
    "To do that, we can apply several discretization form. The one chosen by the authors is the bilinear method, that states that instead of using $\\bA$ and $\\bb$, for discrete inputs we can use $\\bAb$ and $\\bbb$ introduced earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HiPPO matrix canonical basis\n",
    "\n",
    "Let's write some functions to compute $\\bA$ and $\\bb$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_matrices(N):\n",
    "    u = torch.arange(N)\n",
    "    b = torch.sqrt(2 * u[:, None] + 1)\n",
    "    A = b @ b.T\n",
    "    A = torch.tril(A, 0)\n",
    "    A = -(A - torch.diag(u))\n",
    "\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [-1.7321, -2.0000, -0.0000, -0.0000],\n",
       "        [-2.2361, -3.8730, -3.0000, -0.0000],\n",
       "        [-2.6458, -4.5826, -5.9161, -4.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, b = hippo_matrices(4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HiPPO DPLR\n",
    "\n",
    "We have talked later about the DPLR form. We will now that indeed $\\bA$ given by the HiPPO framework is in fact DPLR.\n",
    "\n",
    "Indeed, one can notice that if $\\ptilde = \\dfrac{1}{\\sqrt{2}}\\bb$, then:\n",
    "\n",
    "\\begin{align}\n",
    "    \\bA + \\ptilde \\ptilde^t &= -\\begin{cases}\n",
    "        \\dfrac{1}{2}(2i + 1)^{1/2}(2j + 1)^{1/2} &\\quad \\text{if} \\, i > j,\\\\\n",
    "        \\dfrac{1}{2} &\\quad \\text{if} \\, i = j,\\\\\n",
    "        -\\dfrac{1}{2}(2i + 1)^{1/2}(2j + 1)^{1/2} &\\quad \\text{if} \\, i < j,\n",
    "    \\end{cases}\\\\\n",
    "    &= -\\dfrac{1}{2}\\bI - \\bS\n",
    "\\end{align}\n",
    "where $\\bS$ is a skew-symmetric matrix ($\\bS = - \\bS^*$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\bS =\n",
    "\\begin{cases}\n",
    "        \\dfrac{1}{2}(2i + 1)^{1/2}(2j + 1)^{1/2} &\\quad \\text{if} \\, i > j,\\\\\n",
    "        0 &\\quad \\text{if} \\, i = j,\\\\\n",
    "        -\\dfrac{1}{2}(2i + 1)^{1/2}(2j + 1)^{1/2} &\\quad \\text{if} \\, i < j,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "And a skew-symmetric matrix can be diagonalized (in $\\CC$), hence it means that $\\bA$ is similar to a DPLR matrix!\n",
    "\n",
    "\\begin{equation}\n",
    "\\exists \\bV \\in \\mathcal{O}_N(\\CC),\\, \\bA = \\bV \\left( \\bLa - \\bp \\bp^* \\right) \\bV^*,\n",
    "\\end{equation}\n",
    "\n",
    "with $\\bV$ the matrix representing the (complex) eigenvectors of $\\bS$, $\\mathcal{O}_N(\\CC)$ the set of orthogonal matrices of size $N$ in $\\CC$, and $\\bp = \\bV^* \\ptilde$.\n",
    "Note that we should not forget to also change the basis of $\\bb$ when we want to use the special form $\\bA = \\bLa - \\bp \\bp^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_matrices_dplr(N):\n",
    "    u = torch.arange(N)[:, None]\n",
    "\n",
    "    p_tilde = torch.sqrt(u + 0.5)\n",
    "\n",
    "    # Construct S\n",
    "    S = torch.tril(p_tilde @ p_tilde.T)\n",
    "    S = -S + S.T\n",
    "\n",
    "    # A small trick to make a skew-hermitian matrix a hermitian one by only multiplying its eigenvalues by a constant.\n",
    "    hermitian_S = S * -1j\n",
    "\n",
    "    # Diagonalize S and extract V.\n",
    "    Lambda, V = torch.linalg.eigh(hermitian_S)\n",
    "\n",
    "    # Mutliplies back the eigenvalues by (1j)^-1 to retrieve the original eigenvalues of the skew-hermitian matrix.\n",
    "    # We have to add the real parts of the eigenvalues, coming for the 1/2*Id part of the decomposition.\n",
    "    Lambda = Lambda * 1j - 0.5\n",
    "\n",
    "    b = torch.sqrt(2 * u + 1)\n",
    "\n",
    "    # Change of basis for b and p.\n",
    "    b = V.H @ b.type(torch.complex64)\n",
    "    p = V.H @ p_tilde.type(torch.complex64)\n",
    "\n",
    "    return V, Lambda, p, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, Lambda, p, b = hippo_matrices_dplr(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we did not make any mistakes in our change of basis by comparing $\\bA$ obtained from the direct formula and the one obtained through diagonalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = hippo_matrices(N)\n",
    "\n",
    "assert torch.allclose(\n",
    "    V @ (torch.diag(Lambda) - p @ p.H) @ V.H, A.type(torch.complex64), atol=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything to initialize our parameters:\n",
    "\n",
    "- Create $\\bA, \\bp$ and $\\bb$ with their closed form expressions.\n",
    "- Instead of optimizing $\\bc$ we are going to optimize $\\ctilde$ which is in fact the only part where $\\bc$ is used. It will help reducing the number of operations and especially avoid exponentiating $\\bA$ (this is however a bit incorrect because we are losing the property that the output is real, but we can hope the neural network will learn a 'good' $\\ctilde$ which in the end has this property).\n",
    "- Initialize $\\ctilde$ at random.\n",
    "- Extract $\\bS$ from $\\bA$ and compute its eigenbasis.\n",
    "- Change the basis of $\\bp, \\bb$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting to the full implementation, we are going to explain how we can perform autoregressive decoding.\n",
    "Indeed, when doing a convolution, we assume that the whole input in known. In autoregressive decoding it is not the case anymore. We therefore have to get back to the basic definition of the state-space system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent decoding\n",
    "The authors derived a computation of each step in $\\mathcal{O}(N)$ (instead of a naive $\\mathcal{O}(N^2))$, by leveraging the DPLR form of $\\bA$ with single unidimensional inputs, and $\\mathcal{O}(HN)$ with H-dimensional inputs and $N$ the polynomial space dimension.\n",
    "\n",
    "The authors derive that we can decompose $\\bAb$ as the product of two DPLR matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bAb = \\bA_1 \\bA_0,\n",
    "\\end{equation}\n",
    "with:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bA_i = \\bLa_i - \\bp_i \\bq_i^*.  \n",
    "\\end{equation}\n",
    "where:\n",
    "\\begin{cases}\n",
    "    \\bLa_0 &= 2 + \\bLa,\\\\\n",
    "    \\bp_0 &= \\bp,\\\\\n",
    "    \\bq_0 &= \\bp.\n",
    "\\end{cases}\n",
    "and:\n",
    "\\begin{cases}\n",
    "    \\bLa_1 &= (2 - \\bLa)^{-1},\\\\\n",
    "    \\bp_1 &= \\bLa_1(1 + \\bp^* \\bLa_1 \\bp)^{-1},\\\\\n",
    "    \\bq_1 &= \\bp^* \\bLa_1\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive matrices computations\n",
    "Below is the values of the matrices computed in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_dplr(Lambda, p, b, step=1.0):\n",
    "    # Convert parameters to matrices\n",
    "    N = Lambda.shape[0]\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    I = torch.eye(N)\n",
    "\n",
    "    # Forward Euler\n",
    "    A_0 = (2.0 / step) * I + A\n",
    "\n",
    "    # Backward Euler\n",
    "    D = torch.diag(1.0 / ((2.0 / step) - Lambda))\n",
    "    D_p = D @ p\n",
    "\n",
    "    A_1 = D - (D_p * (1.0 / (1 + (p.H @ D_p))) * p.H @ D)\n",
    "\n",
    "    # A bar and b bar\n",
    "    A_bar = A_1 @ A_0\n",
    "    b_bar = 2 * A_1 @ b\n",
    "\n",
    "    return A_bar, b_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn(N, 1)\n",
    "b = torch.randn(N, 1)\n",
    "Lambd = torch.randn(N)\n",
    "\n",
    "# Our new formula\n",
    "A_bar_dplr, b_bar_dplr = discrete_dplr(Lambd, p, b)\n",
    "\n",
    "# Base formula\n",
    "A = torch.diag(Lambd) - p @ p.H\n",
    "A_bar_closed_form, b_bar_closed_form = discretize(A, b)\n",
    "\n",
    "assert torch.allclose(A_bar_dplr, A_bar_closed_form, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform autoregressice decoding easily, simply by using our matrices $\\bA_0, \\ \\bA_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_ssm(A_bar, b_bar, c, u, x_0):\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = A_bar @ x_k_1 + b_bar @ u_k\n",
    "        y_k = c.H @ x_k\n",
    "        return x_k, y_k\n",
    "\n",
    "    recurrence = []\n",
    "    x_k = x_0\n",
    "    for i in range(u.shape[0]):\n",
    "        x_k, y_k = step(x_k, u[i])\n",
    "        recurrence.append(y_k)\n",
    "    return x_k, torch.tensor(recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient implentation\n",
    "\n",
    "But with this form we do not leverage the efficiency of DPLR matrices. Indeed, the matrix-vector product of a DPLR matrix is $\\mathcal{O}(N)$.\n",
    "Indeed:\n",
    "\n",
    "\\begin{equation}\n",
    "    (\\bLa - \\bp \\bq^*)\\bx = \\bLa \\bx - \\bp \\underbrace{(\\bq^* \\bx)}_{scalar}.\n",
    "\\end{equation}\n",
    "\n",
    "To code this, we are simple going to need the DPLR decomposition and not the full matrix anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_dplr_opti(Lambda, p, b, step=1):\n",
    "    # Lambda (N,)\n",
    "    # p, p (N, 1)\n",
    "    \"\"\"DPLR of A_0 and A_1.\"\"\"\n",
    "    Lambda_0 = 2.0 / step + Lambda[:, None]\n",
    "    p_0 = p\n",
    "    q_0 = p\n",
    "\n",
    "    Lambda_1 = 1.0 / ((2.0 / step) - Lambda[:, None])\n",
    "    p_1 = Lambda_1 * p * 1.0 / (1.0 + p.H @ (Lambda_1 * p))\n",
    "    q_1 = Lambda_1.conj() * p\n",
    "\n",
    "    b_bar = 2 * (Lambda_1 * b - p_1 * (q_1.H @ b))\n",
    "\n",
    "    return (Lambda_0, p_0, q_0), (Lambda_1, p_1, q_1), b_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dplr_vector_product(dplr, x):\n",
    "    \"\"\"Unidimensionnal optimized dplr matrix-vector product.\"\"\"\n",
    "    Lambda, p, q = dplr\n",
    "    diagonal_prod = Lambda * x\n",
    "\n",
    "    lr_prod = p * (q.H @ x)\n",
    "\n",
    "    return diagonal_prod - lr_prod\n",
    "\n",
    "\n",
    "def scan_ssm_opti(dplr_0, dplr_1, b_bar, c, u, x_0):\n",
    "    \"\"\"Optimized RNN computations.\"\"\"\n",
    "\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = dplr_vector_product(dplr_0, x_k_1)\n",
    "        x_k = dplr_vector_product(dplr_1, x_k) + b_bar * u_k\n",
    "        y_k = c.H @ x_k\n",
    "\n",
    "        return x_k, y_k\n",
    "\n",
    "    recurrence = []\n",
    "    x_k = x_0\n",
    "    for i in range(u.shape[0]):\n",
    "        x_k, y_k = step(x_k, u[i])\n",
    "        recurrence.append(y_k)\n",
    "    return x_k, torch.tensor(recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a big check, to see if everything is coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conversion(N=8, L=16):\n",
    "    V, Lambda, p, b = hippo_matrices_dplr(N)\n",
    "\n",
    "    # Compute A in closed form\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "    c_base = torch.randn(N, 1)\n",
    "\n",
    "    # Change the basis of c.\n",
    "    c = V.H @ c_base.type(torch.complex64)\n",
    "    A_bar, b_bar = discretize(A, b)\n",
    "    c_tilde = compute_c_tilde(A_bar, c, L)\n",
    "\n",
    "    # CNN form.\n",
    "    K = kernel_dplr(Lambda, p, b, c_tilde, L)\n",
    "\n",
    "    # Simple RNN form.\n",
    "    K_simple = kernel_simple(A_bar, b_bar, c, L=L)\n",
    "    assert torch.allclose(K, K_simple, atol=1e-4)\n",
    "\n",
    "    # New RNN form\n",
    "    A_bar_new, b_bar_new = discrete_dplr(Lambda, p, b)\n",
    "    K_rnn = kernel_simple(A_bar, b_bar, c, L=L)\n",
    "    assert torch.allclose(K_simple, K_rnn, atol=1e-4)\n",
    "\n",
    "    # Apply CNN\n",
    "    u = torch.arange(L).type(torch.complex64)\n",
    "    y_1 = causal_convolution(u, K)\n",
    "\n",
    "    # Apply RNN\n",
    "    _, y_2 = scan_ssm(\n",
    "        A_bar, b_bar, c, u[:, None], torch.zeros((N,)).type(torch.complex64)\n",
    "    )\n",
    "    assert torch.allclose(y_1, y_2.reshape(-1), atol=1e-4, rtol=1e-4)\n",
    "\n",
    "    # Optimized RNN form\n",
    "    dplr_0, dplr_1, b_bar_opti = discrete_dplr_opti(Lambda, p, b)\n",
    "\n",
    "    assert torch.allclose(b_bar, b_bar_opti)\n",
    "\n",
    "    _, y_2_opti = scan_ssm_opti(\n",
    "        dplr_0,\n",
    "        dplr_1,\n",
    "        b_bar_opti,\n",
    "        c,\n",
    "        u[:, None],\n",
    "        torch.zeros((N, 1)).type(torch.complex64),\n",
    "    )\n",
    "    assert torch.allclose(y_1, y_2_opti.reshape(-1), atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi dimensional recurrent decoding\n",
    "We are almost done. We now need to adapt it to handle multidimensional inputs, as well as a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_dplr_opti_multi(Lambda, p, b, step=1):\n",
    "    \"\"\"DPLR of A_0 and A_1 with multidimensionnal data.\"\"\"\n",
    "\n",
    "    # Lambda (H, N)\n",
    "    # p, b (H, N)\n",
    "    Lambda_0 = 2.0 / step + Lambda\n",
    "    p_0 = p\n",
    "    q_0 = p\n",
    "\n",
    "    Lambda_1 = 1.0 / ((2.0 / step) - Lambda)\n",
    "    p_1 = (\n",
    "        Lambda_1\n",
    "        * p\n",
    "        * 1.0\n",
    "        / (1.0 + (p.conj() * Lambda_1 * p).sum(axis=-1, keepdim=True))\n",
    "    )\n",
    "    q_1 = Lambda_1.conj() * p\n",
    "\n",
    "    b_bar = 2 * (Lambda_1 * b - p_1 * (q_1.conj() * b).sum(axis=-1, keepdim=True))\n",
    "\n",
    "    return (Lambda_0, p_0, q_0), (Lambda_1, p_1, q_1), b_bar\n",
    "\n",
    "\n",
    "def dplr_vector_product_multi(dplr, x):\n",
    "    # x (B, H, N)\n",
    "    # Lambda (H, N)\n",
    "    # p, q (H, N)\n",
    "\n",
    "    Lambda, p, q = dplr\n",
    "    diagonal_prod = Lambda[None, :, :] * x\n",
    "    lr_prod = p[None, :, :] * (q[None, :, :].conj() * x).sum(axis=2, keepdims=True)\n",
    "\n",
    "    return diagonal_prod - lr_prod\n",
    "\n",
    "\n",
    "def dplr_vector_prod_multi_simple(dplr, x):\n",
    "    \"\"\"Naive version of dplr matrix-vector product.\"\"\"\n",
    "    out = torch.zeros_like(x)\n",
    "    Lambda, p, q = dplr\n",
    "    for i in range(x.shape[0]):\n",
    "        for h in range(x.shape[1]):\n",
    "            prod = dplr_vector_product(\n",
    "                (Lambda[h][:, None], p[h][:, None], q[h][:, None]), x[i, h][:, None]\n",
    "            ).view(-1)\n",
    "            out[i, h] = prod\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dplr_random_multi(H, N):\n",
    "    \"\"\"Returns a dplr represented by a three vectors.\"\"\"\n",
    "    Lambda = torch.randn(H, N, dtype=torch.complex64)\n",
    "    p = torch.randn(H, N, dtype=torch.complex64)\n",
    "    q = torch.randn(H, N, dtype=torch.complex64)\n",
    "    return Lambda, p, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3\n",
    "N = 5\n",
    "x = torch.randn(8, H, N, dtype=torch.complex64)\n",
    "Lambda, p, q = make_dplr_random_multi(H, N)\n",
    "dplr = Lambda, p, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if our vectorized dplr matrix vector product works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dplr_prod = dplr_vector_product_multi(dplr, x)\n",
    "dplr_prod_simple = dplr_vector_prod_multi_simple(dplr, x)\n",
    "\n",
    "assert torch.allclose(dplr_prod, dplr_prod_simple, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_ssm_opti_multi(dplr_0, dplr_1, b_bar, c, u, x_0):\n",
    "    # b_bar (H, N)\n",
    "    # c (H, N)\n",
    "    # u (B, H, L)\n",
    "    # x_0 (B, H, N)\n",
    "\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = dplr_vector_product_multi(dplr_0, x_k_1)\n",
    "\n",
    "        x_k = (\n",
    "            dplr_vector_product_multi(dplr_1, x_k) + b_bar[None, :, :] * u_k[:, :, None]\n",
    "        )\n",
    "        y_k = (c[None, :, :].conj() * x_k).sum(axis=-1, keepdim=True)\n",
    "\n",
    "        return x_k, y_k\n",
    "\n",
    "    recurrence = []\n",
    "    x_k = x_0\n",
    "    for i in range(u.shape[-1]):\n",
    "        x_k, y_k = step(x_k, u[:, :, i])\n",
    "        recurrence.append(y_k)\n",
    "\n",
    "    return x_k, torch.cat(recurrence, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_ssm_opti_multi_simple(dplr_0, dplr_1, b_bar, c, u, x_0):\n",
    "    \"\"\"Naive version of scan ssm multi.\"\"\"\n",
    "    Lambda_0, p_0, q_0 = dplr_0\n",
    "    Lambda_1, p_1, q_1 = dplr_1\n",
    "\n",
    "    batch_size = u.shape[0]\n",
    "    hidden_size = u.shape[1]\n",
    "\n",
    "    out = torch.zeros_like(u)\n",
    "    for i in range(batch_size):\n",
    "        for h in range(hidden_size):\n",
    "            dplr_h_0 = Lambda_0[h][:, None], p_0[h][:, None], q_0[h][:, None]\n",
    "            dplr_h_1 = Lambda_1[h][:, None], p_1[h][:, None], q_1[h][:, None]\n",
    "            out[i, h] = scan_ssm_opti(\n",
    "                dplr_h_0,\n",
    "                dplr_h_1,\n",
    "                b_bar[h][:, None],\n",
    "                c[h][:, None],\n",
    "                u[i, h][:, None],\n",
    "                x_0[i, h][:, None],\n",
    "            )[-1]\n",
    "    return None, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 5\n",
    "N = 10\n",
    "B = 4\n",
    "L = 3\n",
    "dplr_0 = make_dplr_random_multi(H, N)\n",
    "dplr_1 = make_dplr_random_multi(H, N)\n",
    "x_0 = torch.zeros(B, H, N, dtype=torch.complex64)\n",
    "u = torch.randn(B, H, L, dtype=torch.complex64)\n",
    "b_bar = torch.randn(H, N, dtype=torch.complex64)\n",
    "c = torch.randn(H, N, dtype=torch.complex64)\n",
    "\n",
    "_, scan_simple = scan_ssm_opti_multi_simple(dplr_0, dplr_1, b_bar, c, u, x_0)\n",
    "_, scan = scan_ssm_opti_multi(dplr_0, dplr_1, b_bar, c, u, x_0)\n",
    "\n",
    "assert torch.allclose(scan, scan_simple, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conversion_multi(B=4, H=10, N=8, L=5):\n",
    "    V, Lambda, p, b = hippo_matrices_dplr(N)\n",
    "\n",
    "    # Compute A in closed form.\n",
    "    A = torch.diag(Lambda) - p @ p.H\n",
    "\n",
    "    # Make H copies of A, p, b, Lambda.\n",
    "    A = A.expand(H, N, N).clone()\n",
    "    p = p.squeeze().expand(H, N).clone()\n",
    "    b = b.squeeze().expand(H, N).clone()\n",
    "    Lambda = Lambda.squeeze().expand(H, N).clone()\n",
    "\n",
    "    # Initialize H c.\n",
    "    c_base = torch.randn(H, N)\n",
    "\n",
    "    # Change the basis of c.\n",
    "    c = V.H @ c_base[:, :, None].type(torch.complex64)\n",
    "    c = c.view(H, N)\n",
    "\n",
    "    # Get A_bar (needed for c_tilde).\n",
    "    A_bar, b_bar = discretize_multi(A, b)\n",
    "\n",
    "    # Compute c_tilde\n",
    "    c_tilde = compute_c_tilde_multi(A_bar, c, L)\n",
    "\n",
    "    # CNN form. Check again if our vectorized computations give the same result as a naive one.\n",
    "    K = kernel_dplr_multi(Lambda, p, b, c_tilde, L)\n",
    "    K_simple = kernel_dplr_multi_simple(\n",
    "        Lambda, p[:, :, None], b[:, :, None], c_tilde[:, :, None], L\n",
    "    )\n",
    "\n",
    "    assert torch.allclose(K, K_simple, atol=1e-4)\n",
    "\n",
    "    # Create an input signal with a batch size, a hidden size and a sequence length.\n",
    "\n",
    "    u = torch.randn(B, H, L).type(torch.complex64)\n",
    "\n",
    "    # Apply CNN\n",
    "    y_cnn = causal_convolution_multi(u, K)\n",
    "\n",
    "    # Apply RNN\n",
    "    # Optimized RNN form\n",
    "    dplr_0, dplr_1, b_bar_opti = discrete_dplr_opti_multi(Lambda, p, b)\n",
    "\n",
    "    # Check if we obtain the same b_bar than with the closed form formula.\n",
    "    assert torch.allclose(b_bar, b_bar_opti)\n",
    "\n",
    "    # Apply RNN autoregressively,\n",
    "    _, y_rnn = scan_ssm_opti_multi(\n",
    "        dplr_0, dplr_1, b_bar_opti, c, u, torch.zeros((H, N)).type(torch.complex64)\n",
    "    )\n",
    "\n",
    "    # Compare with CNN output.\n",
    "    assert torch.allclose(y_cnn, y_rnn, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conversion_multi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We now have everything to implement our model within a Pytorch module. We remind here the the initialization steps.\n",
    "As well as the functions we will need in our final implementation.\n",
    "\n",
    "# Full  layer\n",
    "- Create $\\bA, \\bp$ and $\\bb$ with their closed form expressions.\n",
    "- Instead of optimizing $\\bc$ we are going to optimize $\\ctilde$ which is in fact the only part where $\\bc$ is used. It will help reducing the number of operations and especially avoid exponentiating $\\bA$ (this is however a bit incorrect because we are losing the property that the output is real, but we can hope the neural network will learn a 'good' $\\ctilde$ which in the end has this property).\n",
    "- Initialize $\\ctilde$ at random.\n",
    "- Extract $\\bS$ from $\\bA$ and compute its eigenbasis.\n",
    "- Change the basis of $\\bp, \\bb$.\n",
    "- Initialize at $\\bD$ random that we are going to use as a \"skip connection\".\n",
    "\n",
    "In addition to the base model we used until now, we are going to introduce a \"skip-connection\" that is actually part of the rigorous definition of state-space models:\n",
    "\n",
    "\n",
    "$y_k = \\bc x_k + \\bD_k u_k$, where $\\bD \\in \\RR^L$ (or $\\in \\RR^{HL}$).\n",
    "- Initialize $\\bD$ at random that we are going to use as a \"skip connection\".\n",
    "\n",
    "From now all our vectors are going to be complex.\n",
    "\n",
    "Let's write that in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_tensor(x, expand_size):\n",
    "    clone = x.detach().expand(expand_size, *x.shape).clone()\n",
    "\n",
    "    clone.requires_grad_()\n",
    "    return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_convolution_multi(u, K):\n",
    "    fft_shape = u.shape[2] + K.shape[1]\n",
    "    u_fft = torch.fft.fft(u, n=fft_shape, axis=-1)\n",
    "    K_fft = torch.fft.fft(K, n=fft_shape, axis=-1)\n",
    "    out = u_fft * K_fft\n",
    "    return torch.fft.ifft(out, axis=-1)[:, :, : u.shape[2]]\n",
    "\n",
    "\n",
    "def discretize_multi(A, b, step=1):\n",
    "    I = torch.eye(A.shape[1]).to(A.device)\n",
    "    left_term = torch.linalg.inv(I[None, :, :] - (step / 2.0) * A)\n",
    "    A_bar = left_term @ (I + (step / 2.0) * A)\n",
    "    b_bar = (left_term * step) @ b[:, :, None]\n",
    "    return A_bar, b_bar.reshape(b.shape[0], -1)\n",
    "\n",
    "\n",
    "def compute_c_multi(A, c_tilde, L):\n",
    "    correction = torch.eye(A.shape[1]).to(A.device)[None, :, :] - A.matrix_power(L)\n",
    "\n",
    "    c = torch.linalg.inv(correction.mH) @ c_tilde[:, :, None]\n",
    "    return c.view(c.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_ssm_opti_multi(dplr_0, dplr_1, b_bar, c, u, x_0):\n",
    "    # b_bar (H, N)\n",
    "    # c (H, N)\n",
    "    # u (B, H, L)\n",
    "    # x_0 (B, H, N)\n",
    "\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = dplr_vector_product_multi(dplr_0, x_k_1)\n",
    "\n",
    "        x_k = (\n",
    "            dplr_vector_product_multi(dplr_1, x_k) + b_bar[None, :, :] * u_k[:, :, None]\n",
    "        )\n",
    "        y_k = (c[None, :, :].conj() * x_k).sum(axis=-1, keepdim=True)\n",
    "\n",
    "        return x_k, y_k\n",
    "\n",
    "    recurrence = []\n",
    "    x_k = x_0\n",
    "    for i in range(u.shape[-1]):\n",
    "        x_k, y_k = step(x_k, u[:, :, i])\n",
    "        recurrence.append(y_k)\n",
    "\n",
    "    return x_k, torch.cat(recurrence, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size=256,\n",
    "        embedding_size=128,\n",
    "        sequence_length=512,\n",
    "        ffn_size=4096,\n",
    "        frozen=False,\n",
    "        init_std=0.02,\n",
    "        dropout=0.1,\n",
    "        decode=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.frozen = frozen\n",
    "        self.decode = decode\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dropout = dropout\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        # Layer normalisation.\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        V, Lambda, p, b = hippo_matrices_dplr(self.state_size)\n",
    "\n",
    "        Lambda = clone_tensor(Lambda.view(-1), embedding_size)\n",
    "        p = clone_tensor(p.view(-1), embedding_size)\n",
    "        b = clone_tensor(b.view(-1), embedding_size)\n",
    "\n",
    "        self.Lambda = nn.Parameter(Lambda)\n",
    "        self.p = nn.Parameter(p)\n",
    "        self.b = nn.Parameter(b)\n",
    "\n",
    "        self.c_tilde = nn.Parameter(\n",
    "            init_std * torch.randn(embedding_size, state_size, dtype=torch.complex64)\n",
    "        )\n",
    "        self.D = nn.Parameter(\n",
    "            init_std * torch.randn(1, embedding_size, sequence_length)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_size, ffn_size)\n",
    "        self.fc2 = nn.Linear(ffn_size, embedding_size)\n",
    "\n",
    "    def cauchy_multi(self, p, q, Lambda, omega_L):\n",
    "        \"\"\"Multidimensionnal Cauchy product. Basically we are just using broadcasting to generalize above function.\"\"\"\n",
    "        # p, q, Lambd (H, N)\n",
    "        # omega_L (L,)\n",
    "        omega_L = 2.0 * ((1.0 - omega_L) / (1 + omega_L))\n",
    "        dot_product = p.conj() * q\n",
    "\n",
    "        Lambda = Lambda[:, :, None]\n",
    "        omega_L = omega_L[None, None, :]\n",
    "        cauchy_product = dot_product[:, :, None] / (omega_L - Lambda)\n",
    "        return cauchy_product.sum(axis=-2)\n",
    "\n",
    "    def kernel_dplr_multi(self, Lambda, p, b, c_tilde, L):\n",
    "        omega_L = torch.exp((-2j * torch.pi) * torch.arange(L) / L).to(Lambda.device)\n",
    "        term_1 = self.cauchy_multi(c_tilde, b, Lambda, omega_L)\n",
    "        term_2 = self.cauchy_multi(c_tilde, p, Lambda, omega_L)\n",
    "        term_3 = 1.0 / (1.0 + self.cauchy_multi(p, p, Lambda, omega_L))\n",
    "        term_4 = self.cauchy_multi(p, b, Lambda, omega_L)\n",
    "\n",
    "        K_fft = (2.0 / (1.0 + omega_L[None, :])) * (term_1 - term_2 * term_3 * term_4)\n",
    "        return torch.fft.ifft(K_fft, L)\n",
    "\n",
    "    def set_decoding_mode(self):\n",
    "        \"\"\"When using recurrent decoding mode one needs to define carefully the parameters we are going to use.\"\"\"\n",
    "\n",
    "        self.decode = True\n",
    "\n",
    "        # DPLR decomposition for efficient recurrence.\n",
    "        dplr_0, dplr_1, b_bar = discrete_dplr_opti_multi(self.Lambda, self.p, self.b)\n",
    "        self.dplr_0 = dplr_0\n",
    "        self.dplr_1 = dplr_1\n",
    "        self.b_bar = b_bar\n",
    "\n",
    "        # Compute A\n",
    "        I = torch.eye(self.state_size, dtype=torch.complex64).to(self.b.device)\n",
    "        I = I[None, :, :]\n",
    "        diag_part = self.Lambda[:, :, None] * I\n",
    "        pp = self.p[:, :, None] @ self.p[:, :, None].mH\n",
    "        A = diag_part - pp\n",
    "\n",
    "        # Discretize A\n",
    "        A_bar, b_bar_discrete = discretize_multi(A, self.b)\n",
    "\n",
    "        # Compute back c from c_tilde (remember that we learn c_tilde instead of c).\n",
    "        c = compute_c_multi(A_bar, self.c_tilde, L)\n",
    "        self.c = c\n",
    "\n",
    "        # Initial state.\n",
    "        self.x_k = torch.zeros(\n",
    "            self.embedding_size, self.state_size, dtype=torch.complex64\n",
    "        ).to(self.b.device)\n",
    "\n",
    "    def forward(self, u, attention_mask=None):\n",
    "\n",
    "        if self.decode:\n",
    "            x_k, out = scan_ssm_opti_multi(\n",
    "                self.dplr_0, self.dplr_1, self.b_bar, self.c, u, self.x_k\n",
    "            )\n",
    "            self.x_k = x_k\n",
    "\n",
    "            # We take the real part for simplicity because all Pytorch modules doesn't support complex inputs.\n",
    "            # We can imagine a setup where one sticks with complex numbers but in this case we have to manually write the layers.\n",
    "            out = out.real\n",
    "            # Transpose to (B, L, H) because Pytorch Linear operates on the last dimension of tensors.\n",
    "            out_transposed = out.transpose(-1, -2)\n",
    "            residual = out_transposed\n",
    "\n",
    "            out_transposed = self.activation(self.fc1(out_transposed))\n",
    "            out_transposed = F.dropout(\n",
    "                out_transposed, p=self.dropout, training=self.training\n",
    "            )\n",
    "            out_transposed = self.activation(self.fc2(out_transposed))\n",
    "            out_transposed = F.dropout(\n",
    "                out_transposed, p=self.dropout, training=self.training\n",
    "            )\n",
    "            # Layer normalization\n",
    "            out_transposed = self.norm(out_transposed + residual)\n",
    "\n",
    "            # Transpose back to (B, H, L)\n",
    "            out = out_transposed.transpose(-1, -2)\n",
    "            return out + self.D * u\n",
    "\n",
    "        else:\n",
    "            K = self.kernel_dplr_multi(\n",
    "                self.Lambda, self.p, self.b, self.c_tilde, self.sequence_length\n",
    "            )\n",
    "\n",
    "            # We take the real part for simplicity because all Pytorch modules doesn't support complex inputs.\n",
    "            # We can imagine a setup where one sticks with complex numbers but in this case we have to manually write the layers.\n",
    "            out = causal_convolution_multi(u, K).real\n",
    "\n",
    "            # Transpose to (B, L, H) because Pytorch Linear operates on the last dimension of tensors.\n",
    "            out_transposed = out.transpose(-1, -2)\n",
    "            residual = out_transposed\n",
    "\n",
    "            out_transposed = self.activation(self.fc1(out_transposed))\n",
    "            out_transposed = F.dropout(\n",
    "                out_transposed, p=self.dropout, training=self.training\n",
    "            )\n",
    "            out_transposed = self.activation(self.fc2(out_transposed))\n",
    "            out_transposed = F.dropout(\n",
    "                out_transposed, p=self.dropout, training=self.training\n",
    "            )\n",
    "            # Layer normalization\n",
    "            out_transposed = self.norm(out_transposed + residual)\n",
    "\n",
    "            # Transpose back to (B, H, L)\n",
    "            out = out_transposed.transpose(-1, -2)\n",
    "            return out + self.D * u\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers=12,\n",
    "        state_size=256,\n",
    "        embedding_size=128,\n",
    "        sequence_length=512,\n",
    "        ffn_size=3072,\n",
    "        frozen=False,\n",
    "        init_std=0.02,\n",
    "        dropout=0.1,\n",
    "        decode=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                S4Layer(\n",
    "                    state_size,\n",
    "                    embedding_size,\n",
    "                    sequence_length=sequence_length,\n",
    "                    ffn_size=ffn_size,\n",
    "                    dropout=dropout,\n",
    "                    frozen=frozen,\n",
    "                    init_std=init_std,\n",
    "                    decode=decode,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def set_decoding_mode(self):\n",
    "        for layer in self.layers:\n",
    "            layer.set_decoding_mode()\n",
    "\n",
    "    def forward(self, u):\n",
    "        hidden_states = u\n",
    "        for idx, s4_layer in enumerate(self.layers):\n",
    "            hidden_states = s4_layer(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our final model. We modified a little bit the functions to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S4Layer(\n",
       "  (activation): GELU()\n",
       "  (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=2, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 10\n",
    "H = 2\n",
    "N = 3\n",
    "s4 = S4Layer(state_size=N, embedding_size=H, sequence_length=L)\n",
    "s4.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.randn(4, H, L).to(device)\n",
    "y_conv = s4(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recurrent decoding\n",
    "s4.set_decoding_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_recurrent = s4(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(y_conv, y_recurrent, atol=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-s4",
   "language": "python",
   "name": "env-s4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a263f42c39b2fd75b37cb89946a27f6140cd004b337ff228565a862a637fff96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
